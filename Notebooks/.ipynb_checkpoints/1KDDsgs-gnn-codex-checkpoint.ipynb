{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZGUfQ1TB86e",
    "outputId": "fda95a5b-cff2-45a7-f5b2-c4207aae9666"
   },
   "outputs": [],
   "source": [
    "# %pip install torch_geometric\n",
    "# %pip install ogb\n",
    "# %pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()\n",
    "\n",
    "FIGSIZE = (5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NZtT5Sw_8zb8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid, Reddit\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch_geometric.datasets import WikipediaNetwork\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.typing import Adj, SparseTensor\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "from torch_geometric.utils import scatter\n",
    "from torch_geometric.datasets import Reddit, Reddit2, Flickr, Yelp, AmazonProducts, PPI,  OGB_MAG,  FakeDataset, Amazon,Coauthor,HeterophilousGraphDataset, CitationFull\n",
    "from torch_geometric.utils import homophily\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "def fix_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuE5unEi0s3b",
    "outputId": "82289b7d-95d8-429c-f0bb-4aa797f3de7f"
   },
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import LINKXDataset\n",
    "# dataset_name = 'Roman-empire'\n",
    "# dataset = HeterophilousGraphDataset('/tmp/Cora', DATASET_NAME)\n",
    "# DATASET_NAME = dataset_name\n",
    "# data = dataset[0]\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "ZzPGehcRifbP",
    "outputId": "b26f08a1-e66c-4e3e-9aba-48ff329feb69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoonDataset(10)\n",
      "Data(x=[150, 2], edge_index=[2, 870], y=[150], train_mask=[150], val_mask=[150], test_mask=[150], edge_weight=[870, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import KarateClub, Reddit, Reddit2, Flickr, Yelp, AmazonProducts, PPI,  OGB_MAG,  FakeDataset, Amazon,Coauthor,HeterophilousGraphDataset,LINKXDataset\n",
    "\n",
    "def load_dataset(dataset_name):\n",
    "    if dataset_name == \"karate\":\n",
    "        dataset = KarateClub()\n",
    "    elif dataset_name == \"moon\":\n",
    "        from ipynb.fs.full.Moon import MoonDataset\n",
    "        dataset = MoonDataset(n_samples=150, degree=4, train=0.3, h = 0.2)\n",
    "    elif dataset_name == \"SmallCora\":\n",
    "        dataset = Planetoid(root=DIR+'/tmp/Cora', name='Cora')\n",
    "    elif dataset_name in [\"Cora\", \"Cora_ML\" \"CiteSeer\", \"DBLP\", \"PubMed\"]:\n",
    "        dataset = CitationFull(root=DIR+'/tmp/Citation/'+dataset_name, name=dataset_name)\n",
    "    elif dataset_name == 'Amazon-ratings':\n",
    "        dataset = HeterophilousGraphDataset(root=DIR+'/tmp/amazon_ratings', name = dataset_name)\n",
    "    elif dataset_name == 'Roman-empire':\n",
    "        from torch_geometric.datasets import LINKXDataset\n",
    "        dataset = LINKXDataset(DIR+'/tmp/Roman_empire', dataset_name)\n",
    "    elif dataset_name == \"Reddit\":\n",
    "        dataset = Reddit(root=DIR+'/tmp/Reddit')\n",
    "    elif dataset_name == 'penn94':\n",
    "        from torch_geometric.datasets import LINKXDataset\n",
    "        dataset = LINKXDataset(root=DIR+'/tmp/LINKX', name=dataset_name)\n",
    "    elif dataset_name == 'wiki':\n",
    "        from torch_geometric.datasets import WikiCS\n",
    "        dataset = WikiCS(root=DIR+'/tmp/WikiCS')\n",
    "    elif dataset_name == 'Photo':\n",
    "        dataset = Amazon(root=DIR+'/tmp/Photo', name='Photo')\n",
    "    elif dataset_name == 'CiteSeer':\n",
    "        dataset = Planetoid(root=DIR+'/tmp/CiteSeer', name=DATASET_NAME)\n",
    "    elif dataset_name == 'CS':\n",
    "        dataset = Coauthor(root=DIR+'/tmp/CS', name = DATASET_NAME)\n",
    "    elif dataset_name == 'Physics':\n",
    "        dataset = Coauthor(root=DIR+'/tmp/Physics', name = DATASET_NAME)\n",
    "    elif dataset_name == 'Minesweeper':\n",
    "        dataset   = HeterophilousGraphDataset(root=DIR+'/tmp/Mine',name=dataset_name)\n",
    "    elif dataset_name == 'pokec':\n",
    "        print(\"HELLOWROLD \")\n",
    "    elif dataset_name == 'ogbn-proteins':\n",
    "        dataset = PygNodePropPredDataset(name='ogbn-proteins', root=DIR+'/tmp/ogbn-proteins')\n",
    "        data = dataset[0]\n",
    "        data.node_species = None\n",
    "        data.y = data.y.to(torch.float)\n",
    "\n",
    "        # Initialize features of nodes by aggregating edge features.\n",
    "        row, col = data.edge_index\n",
    "        data.x = scatter(data.edge_attr, col, dim_size=data.num_nodes, reduce='sum')\n",
    "        labels = data.y.argmax(dim=1)\n",
    "        data.y = labels\n",
    "        return dataset, data\n",
    "    elif dataset_name == 'squirrel':\n",
    "        dataset = WikipediaNetwork(root=DIR+'/tmp/squirrel', name='Squirrel')\n",
    "    elif dataset_name == 'AmazonProducts':\n",
    "        dataset = AmazonProducts(root=DIR+'/tmp/AmazonProducts')\n",
    "    else:\n",
    "        dataset = Planetoid(root=DIR+'/tmp/Cora', name='Cora')\n",
    "\n",
    "    print(dataset)\n",
    "    data = dataset[0]\n",
    "    print(data)\n",
    "    return dataset, data\n",
    "\n",
    "# Example usage\n",
    "dataset_name = \"moon\"\n",
    "DATASET_NAME = dataset_name\n",
    "dataset, data = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vUx2t-uxjmV5"
   },
   "outputs": [],
   "source": [
    "# dataset_name = 'Roman-empire'\n",
    "# DATASET_NAME = dataset_name\n",
    "# dataset   = HeterophilousGraphDataset(root='/tmp/roman',name=dataset_name)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gapSL8SlRus",
    "outputId": "a6d02cfc-031b-4d64-9018-3fe9e5648cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Homophily:  0.32413792610168457\n"
     ]
    }
   ],
   "source": [
    "dataset.num_classes\n",
    "He = homophily(data.edge_index, data.y, method='edge')\n",
    "print(\"Edge Homophily: \", He)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jzVprESJYmid"
   },
   "outputs": [],
   "source": [
    "def train_val_test_mask(data, train=0.4, val=0.3, test=0.3, random_state=False):\n",
    "\n",
    "    if isinstance(data.x, SparseTensor):\n",
    "        N = data.x.size(0)\n",
    "        data.num_nodes = N\n",
    "    else:\n",
    "        N = data.x.shape[0]\n",
    "\n",
    "    indexs = list(range(N))\n",
    "\n",
    "    if random_state:\n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test))\n",
    "    else:\n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True\n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"val_mask\" not in data.__dict__['_store']:    \n",
    "    data = train_val_test_mask(data, train=0.2, val=0.4, test=0.4)\n",
    "\n",
    "if len(data.train_mask.shape) > 1:\n",
    "    split_index = 2\n",
    "    data.train_mask = data.train_mask[:,split_index]\n",
    "    data.val_mask = data.val_mask[:,split_index]\n",
    "    data.test_mask = data.test_mask[:,split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP for edge probability with dropout\n",
    "class EdgeProbMLP(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, dropout_prob=0.5):\n",
    "        super(EdgeProbMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(2 * in_channels, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, node_features, edge_index):\n",
    "        edge_features = torch.cat([node_features[edge_index[0]], node_features[edge_index[1]]], dim=1)\n",
    "        x = F.relu(self.fc1(edge_features))\n",
    "        #x = self.dropout(x)\n",
    "        prob = torch.sigmoid(self.fc2(x))\n",
    "        return prob\n",
    "\n",
    "# Define the overall model with Edge Probabilities and GCNConv (GNN) with dropout\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels, num_classes, dropout_prob=0.5):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.edge_prob_mlp = EdgeProbMLP(in_channels, hidden_dim, dropout_prob)\n",
    "        self.gcn1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.gcn2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def get_embeddings(self, data, edge_index, edge_weight=None):\n",
    "        x = F.relu(self.gcn1(data.x, edge_index, edge_weight))\n",
    "        return x\n",
    "\n",
    "    def forward(self, data, edge_index, edge_weight=None):\n",
    "        x = self.get_embeddings(data, edge_index, edge_weight)\n",
    "        out = self.gcn2(x, edge_index, edge_weight)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HGg6LqrCBHVZ"
   },
   "outputs": [],
   "source": [
    "# Gumbel-Softmax trick for edge sampling\n",
    "def gumbel_softmax_sampling(edge_probs, edge_index, q=500, temperature=0.5, log=False, istest=False):\n",
    "        \n",
    "    if istest:\n",
    "        samples = F.softmax(edge_probs/temperature, dim=-1)\n",
    "        top_k_values, sampled_edges = torch.topk(edge_probs, q, dim=-1, largest=True, sorted=True)\n",
    "    else:\n",
    "        samples = F.softmax(edge_probs/temperature, dim=-1)\n",
    "        sampled_edges = torch.multinomial(samples, q, replacement=False) #nothing\n",
    "\n",
    "#     gumbels = -torch.empty_like(edge_probs).exponential_().log()\n",
    "#     gumbels = (gumbels - gumbels.min()) / (gumbels.max() - gumbels.min())\n",
    "#     gumbels = gumbels/10.0\n",
    "\n",
    "#     logits = (edge_probs + gumbels) / temperature    \n",
    "#     samples = F.softmax(logits, dim=-1)\n",
    "\n",
    "#     top_k_values, sampled_edges = torch.topk(samples, q, dim=-1, largest=True, sorted=True) # 50       \n",
    "#     sampled_edges = torch.multinomial(samples, q, replacement=False)\n",
    "#     top_k_values, sampled_edges = torch.topk(edge_probs, q, dim=-1, largest=True, sorted=True)    \n",
    "    \n",
    "    one_hot = torch.zeros_like(samples)\n",
    "    one_hot.scatter_(0, sampled_edges, 1.0)     \n",
    "    \n",
    "    indexs = (one_hot - samples).detach() + samples\n",
    "    \n",
    "#     if log:\n",
    "#         print(\"indexs before: \",indexs)\n",
    "    \n",
    "    edge_probs = edge_probs*indexs\n",
    "    \n",
    "    indexs = indexs.bool()\n",
    "    \n",
    "    \n",
    "#     if log:        \n",
    "#         print(\"edge_probs: \",edge_probs)\n",
    "# #         print(\"gumbels: \", gumbels)\n",
    "# #         print(\"logits: \",logits)\n",
    "#         print(\"samples: \",samples)\n",
    "#         print(\"sampled_edges: \",sampled_edges)\n",
    "#         print(\"One hot: \", one_hot)\n",
    "#         print(\"Indexs: \",indexs)\n",
    "#         print(\"edge_probs[indexs]:\", edge_probs[indexs])\n",
    "\n",
    "    \n",
    "    return indexs, edge_probs[indexs]\n",
    "\n",
    "\n",
    "# Random edge sampling\n",
    "def random_edge_sampling(edge_index, q):\n",
    "    num_edges = edge_index.shape[1]\n",
    "    sampled_indices = torch.randperm(num_edges)[:q]\n",
    "    sampled_edge_index = edge_index[:, sampled_indices]\n",
    "    return sampled_edge_index\n",
    "\n",
    "# F1 score calculation helper\n",
    "def calculate_f1(logits, labels, mask):\n",
    "    preds = logits[mask].argmax(dim=1)\n",
    "    f1 = f1_score(labels[mask].cpu(), preds.cpu(), average='micro')\n",
    "    return f1\n",
    "\n",
    "# Training function with alternating updates\n",
    "def train(epoch, max_epoch, model, optimizer_gnn, optimizer_edge_prob, criterion, cluster_loader, device, q=500, mode='learned', alternate_frequency=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    iteration = 0\n",
    "\n",
    "    for batch in cluster_loader:\n",
    "        if not batch.train_mask.any():\n",
    "            continue\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        if alternate_frequency == 0:\n",
    "            update_gnn = True\n",
    "            update_edge_prob = True\n",
    "        else:            \n",
    "            # Decide if we are updating GNN or edge probability model\n",
    "            update_gnn = iteration % alternate_frequency == 0\n",
    "            update_edge_prob = not update_gnn\n",
    "\n",
    "\n",
    "        # Zero gradients selectively\n",
    "        if update_gnn:\n",
    "            optimizer_gnn.zero_grad()\n",
    "        if update_edge_prob:\n",
    "            optimizer_edge_prob.zero_grad()\n",
    "\n",
    "        # Select edges based on mode\n",
    "        if mode == 'learned':\n",
    "            if batch.edge_index.shape[1] > q:\n",
    "                edge_probs = model.edge_prob_mlp(batch.x, batch.edge_index).squeeze()                    \n",
    "                temperature = 2.0 - (epoch * (2 - 0.5) / max_epoch)\n",
    "                #temperature = 10\n",
    "                #print(temperature)                \n",
    "                sampled_edge_indices, sampled_edge_weight = gumbel_softmax_sampling(edge_probs, batch.edge_index, q=q, temperature=temperature, log=(epoch==max_epoch-1))\n",
    "                sampled_edge_index = batch.edge_index[:, sampled_edge_indices]            \n",
    "                out = model(batch, sampled_edge_index, sampled_edge_weight) \n",
    "                #out = model(batch, sampled_edge_index, None)                                        \n",
    "            else:\n",
    "                out = model(batch, batch.edge_index)\n",
    "        elif mode == 'random':\n",
    "            if batch.edge_index.shape[1] > q:\n",
    "                sampled_edge_index = random_edge_sampling(batch.edge_index, q=q)\n",
    "                out = model(batch, sampled_edge_index)\n",
    "            else:\n",
    "                out = model(batch, batch.edge_index)\n",
    "        elif mode == 'full':\n",
    "            out = model(batch, batch.edge_index)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'learned', 'random', or 'full'.\")\n",
    "\n",
    "        # Compute node classification loss\n",
    "        loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        \n",
    "        if mode == 'learned' and batch.edge_index.shape[1] > q:        \n",
    "            \n",
    "            ########---regularizer 2\n",
    "            \n",
    "            train_indices = torch.nonzero(batch.train_mask).squeeze()            \n",
    "            num_nodes = train_indices.size(0)\n",
    "            num_edges = min(num_nodes*num_nodes,q)  # Adjust the number of edges as needed\n",
    "\n",
    "            # Generate random edges\n",
    "            src = torch.randint(0, num_nodes, (num_edges,))\n",
    "            dst = torch.randint(0, num_nodes, (num_edges,))\n",
    "\n",
    "            # Map to actual node indices\n",
    "            src_nodes = train_indices[src]\n",
    "            dst_nodes = train_indices[dst]\n",
    "\n",
    "            # Create edge_index tensor\n",
    "            reg_edge_index = torch.stack([src_nodes, dst_nodes], dim=0).to(device)\n",
    "            reg_edge_label = (batch.y[src_nodes] == batch.y[dst_nodes]).float().to(device)\n",
    "            \n",
    "            reg_edge_probs = model.edge_prob_mlp(batch.x, reg_edge_index).squeeze()\n",
    "            \n",
    "            loss4 = F.binary_cross_entropy(reg_edge_probs, reg_edge_label)\n",
    "\n",
    "            #print(reg_edge_index)\n",
    "            #print(reg_edge_label)\n",
    "            \n",
    "            loss = loss+loss4\n",
    "            \n",
    "            \n",
    "#             #######---regularizer 1\n",
    "#             edge_labels = torch.full((batch.edge_index.size(1),), -1, dtype=torch.long, device=device)\n",
    "#             train_indices = torch.nonzero(batch.train_mask).squeeze()\n",
    "#             train_edge_mask = torch.isin(batch.edge_index[0], train_indices) & torch.isin(batch.edge_index[1], train_indices)\n",
    "\n",
    "#             # Assign labels based on the class of the endpoints\n",
    "#             same_class_mask = batch.y[batch.edge_index[0]] == batch.y[batch.edge_index[1]]\n",
    "#             edge_labels[train_edge_mask & same_class_mask] = 1\n",
    "#             edge_labels[train_edge_mask & ~same_class_mask] = 0\n",
    "\n",
    "#             # Filter out the edges with labels -1\n",
    "#             valid_edge_mask = edge_labels != -1\n",
    "#             valid_edge_probs = edge_probs[valid_edge_mask]\n",
    "#             valid_edge_labels = edge_labels[valid_edge_mask]\n",
    "            \n",
    "#             #print(torch.sum(valid_edge_labels))\n",
    "#             if torch.sum(valid_edge_labels).item() > 1:            \n",
    "#                 loss2 = F.binary_cross_entropy(valid_edge_probs, valid_edge_labels.float().to(device))\n",
    "#             else:\n",
    "#                 loss2 = 0\n",
    "                \n",
    "#             loss = loss+loss2\n",
    "                        \n",
    "#             #### ----regularizer 3\n",
    "#             neg_q = int(torch.sum(valid_edge_labels).item())            \n",
    "#             neg_q = q\n",
    "#             neg_edge_index = negative_sampling(batch.edge_index, num_nodes=batch.num_nodes, num_neg_samples=neg_q)\n",
    "#             neg_edge_probs = model.edge_prob_mlp(batch.x, neg_edge_index).squeeze()\n",
    "#             neg_edge_labels = torch.zeros(neg_q, dtype=torch.float, device=batch.edge_index.device)            \n",
    "#             loss3 = F.binary_cross_entropy(neg_edge_probs, neg_edge_labels)\n",
    "#             loss = loss+loss3\n",
    "#             \n",
    "            \n",
    "#             #### ------Total Loss            \n",
    "#             print(\"Loss1: \",loss.item(), \" Loss2: \",loss2,\" Loss3: \",loss3)            \n",
    "#             alpha1 = 1.0 \n",
    "#             alpha2 = 1.0 \n",
    "#             loss = loss + alpha1*loss2 + alpha2*loss3                        \n",
    "#             print(\"Total loss: \",loss.item())\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters selectively\n",
    "        if update_gnn:\n",
    "            optimizer_gnn.step()\n",
    "        if update_edge_prob:\n",
    "            optimizer_edge_prob.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        iteration += 1\n",
    "\n",
    "    return total_loss / len(cluster_loader)\n",
    "\n",
    "# Evaluation function remains unchanged\n",
    "def evaluate(model, cluster_loader, device, q=500, mode=None):\n",
    "    model.eval()\n",
    "    total_train_f1, total_val_f1, total_test_f1 = 0, 0, 0\n",
    "    num_train_examples, num_val_examples, num_test_examples = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in cluster_loader:\n",
    "            batch = batch.to(device)\n",
    "            if mode == 'learned':\n",
    "                if batch.edge_index.shape[1] > q:\n",
    "                    edge_probs = model.edge_prob_mlp(batch.x, batch.edge_index).squeeze()                    \n",
    "                    sampled_edge_indices, sampled_edge_weight = gumbel_softmax_sampling(edge_probs, batch.edge_index, q=q, istest=True)\n",
    "                    sampled_edge_index = batch.edge_index[:, sampled_edge_indices]\n",
    "                    out =model(batch, sampled_edge_index, sampled_edge_weight)                                        \n",
    "                    #out = model(batch, sampled_edge_index, None)                                        \n",
    "                else:\n",
    "                    out = model(batch, batch.edge_index)\n",
    "            elif mode == 'random':\n",
    "                if batch.edge_index.shape[1] > q:\n",
    "                    sampled_edge_index = random_edge_sampling(batch.edge_index, q=q)\n",
    "                    out = model(batch, sampled_edge_index)\n",
    "                else:\n",
    "                    out = model(batch, batch.edge_index)\n",
    "            elif mode == 'full':\n",
    "                out = model(batch, batch.edge_index)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode. Choose 'learned', 'random', or 'full'.\")\n",
    "\n",
    "            # Calculate F1 scores for train, val, and test sets if masks are present\n",
    "            if batch.train_mask.any():\n",
    "                train_f1 = calculate_f1(out, batch.y, batch.train_mask)\n",
    "                total_train_f1 += train_f1 * batch.train_mask.sum().item()\n",
    "                num_train_examples += batch.train_mask.sum().item()\n",
    "\n",
    "            if batch.val_mask.any():\n",
    "                val_f1 = calculate_f1(out, batch.y, batch.val_mask)\n",
    "                total_val_f1 += val_f1 * batch.val_mask.sum().item()\n",
    "                num_val_examples += batch.val_mask.sum().item()\n",
    "\n",
    "            if batch.test_mask.any():\n",
    "                test_f1 = calculate_f1(out, batch.y, batch.test_mask)\n",
    "                total_test_f1 += test_f1 * batch.test_mask.sum().item()\n",
    "                num_test_examples += batch.test_mask.sum().item()\n",
    "\n",
    "    avg_train_f1 = total_train_f1 / num_train_examples if num_train_examples > 0 else 0\n",
    "    avg_val_f1 = total_val_f1 / num_val_examples if num_val_examples > 0 else 0\n",
    "    avg_test_f1 = total_test_f1 / num_test_examples if num_test_examples > 0 else 0\n",
    "\n",
    "    return avg_train_f1, avg_val_f1, avg_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge weights given\n"
     ]
    }
   ],
   "source": [
    "if \"edge_weight\" in data.__dict__['_store']:\n",
    "    print(\"Edge weights given\")\n",
    "else:\n",
    "    print(\"No edge weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[150, 2], edge_index=[2, 870], y=[150], train_mask=[150], val_mask=[150], test_mask=[150], edge_weight=[870, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# def adj_feature(data):    \n",
    "#     adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "#     edges = data.edge_index.t()\n",
    "#     adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "#     adj_mat[edges[:,1], edges[:,0]] = 1\n",
    "    \n",
    "# #     return adj_mat\n",
    "    \n",
    "# #     n_components = data.x.shape[1]\n",
    "#     n_components = min(256, data.x.shape[1], data.num_nodes)\n",
    "\n",
    "#     svd = TruncatedSVD(n_components=n_components)\n",
    "#     x = svd.fit_transform(adj_mat)\n",
    "    \n",
    "#     x = torch.Tensor(x)\n",
    "#     x.shape    \n",
    "    \n",
    "#     return x\n",
    "\n",
    "# data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "\n",
    "# # data.x = adj_feature(data)\n",
    "# # print(data.x.shape)\n",
    "# # data.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQoF33fWMAUd"
   },
   "source": [
    "Main for multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTWoJucaYyS0",
    "outputId": "14020a06-a242-4387-a0a2-6bba52c79091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[150, 2], edge_index=[2, 870], y=[150], train_mask=[150], val_mask=[150], test_mask=[150], edge_weight=[870, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = train_val_test_mask(data, train=0.5, val=0.25, test=0.25)\n",
    "print(sum(data.train_mask))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdXts5zdPRso",
    "outputId": "c160ec6b-5dbd-4ae1-fb06-7216c2f9af62",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has fewer than 10,000 nodes; don't do graph partition\n",
      "Run: 0\n",
      "*Epoch 0, model saved with Loss: 1.2890, Train F1: 0.8889, Val F1: 0.8444, Test F1: 0.7167\n",
      "Epoch 0, Loss: 1.2890, Train F1: 0.8889, Val F1: 0.8444, Test F1: 0.7167\n",
      "*Epoch 5, model saved with Loss: 1.2302, Train F1: 0.8667, Val F1: 0.8444, Test F1: 0.7833\n",
      "*Epoch 6, model saved with Loss: 1.1594, Train F1: 0.8667, Val F1: 0.8444, Test F1: 0.8000\n",
      "*Epoch 9, model saved with Loss: 1.0343, Train F1: 0.8667, Val F1: 0.8444, Test F1: 0.8000\n",
      "*Epoch 10, model saved with Loss: 1.0053, Train F1: 0.8889, Val F1: 0.8444, Test F1: 0.7833\n",
      "*Epoch 14, model saved with Loss: 0.8703, Train F1: 0.8889, Val F1: 0.8444, Test F1: 0.7833\n",
      "*Epoch 15, model saved with Loss: 1.0256, Train F1: 0.8667, Val F1: 0.8444, Test F1: 0.8000\n",
      "*Epoch 19, model saved with Loss: 0.9733, Train F1: 0.8889, Val F1: 0.8444, Test F1: 0.8167\n",
      "*Epoch 20, model saved with Loss: 0.8261, Train F1: 0.8889, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 25, model saved with Loss: 0.8653, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 32, model saved with Loss: 0.7773, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8167\n",
      "*Epoch 33, model saved with Loss: 0.7823, Train F1: 0.8889, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 35, model saved with Loss: 0.7633, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.7833\n",
      "*Epoch 36, model saved with Loss: 0.7425, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.7833\n",
      "*Epoch 37, model saved with Loss: 0.7435, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.7833\n",
      "*Epoch 38, model saved with Loss: 0.7545, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 39, model saved with Loss: 0.6524, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 40, model saved with Loss: 0.7881, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 41, model saved with Loss: 0.6965, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8167\n",
      "*Epoch 42, model saved with Loss: 0.7315, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 43, model saved with Loss: 0.6999, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 44, model saved with Loss: 0.7558, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 45, model saved with Loss: 0.7171, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8167\n",
      "*Epoch 46, model saved with Loss: 0.6224, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8333\n",
      "*Epoch 47, model saved with Loss: 0.8041, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8333\n",
      "*Epoch 48, model saved with Loss: 0.7352, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8333\n",
      "*Epoch 49, model saved with Loss: 0.7709, Train F1: 0.9111, Val F1: 0.8667, Test F1: 0.8167\n",
      "*Epoch 50, model saved with Loss: 0.6004, Train F1: 0.9333, Val F1: 0.8667, Test F1: 0.8000\n",
      "*Epoch 51, model saved with Loss: 0.6584, Train F1: 0.9111, Val F1: 0.8889, Test F1: 0.8000\n",
      "*Epoch 52, model saved with Loss: 0.6684, Train F1: 0.9111, Val F1: 0.8889, Test F1: 0.8000\n",
      "*Epoch 53, model saved with Loss: 0.6739, Train F1: 0.9111, Val F1: 0.8889, Test F1: 0.8000\n",
      "*Epoch 60, model saved with Loss: 0.8449, Train F1: 0.9333, Val F1: 0.8889, Test F1: 0.8000\n",
      "*Epoch 61, model saved with Loss: 0.6187, Train F1: 0.9111, Val F1: 0.9111, Test F1: 0.8000\n",
      "*Epoch 62, model saved with Loss: 0.6404, Train F1: 0.9111, Val F1: 0.9111, Test F1: 0.8000\n",
      "*Epoch 63, model saved with Loss: 0.8005, Train F1: 0.9111, Val F1: 0.9111, Test F1: 0.8000\n",
      "*Epoch 72, model saved with Loss: 0.6527, Train F1: 0.9111, Val F1: 0.9111, Test F1: 0.8000\n",
      "*Epoch 76, model saved with Loss: 0.6002, Train F1: 0.9111, Val F1: 0.9111, Test F1: 0.8000\n",
      "*Epoch 86, model saved with Loss: 0.6872, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8000\n",
      "*Epoch 87, model saved with Loss: 0.6697, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8000\n",
      "*Epoch 94, model saved with Loss: 0.7427, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "*Epoch 95, model saved with Loss: 0.6373, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "*Epoch 96, model saved with Loss: 0.7142, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "*Epoch 97, model saved with Loss: 0.6861, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "*Epoch 98, model saved with Loss: 0.7281, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 99, model saved with Loss: 0.5906, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "Epoch 100, Loss: 0.7158, Train F1: 0.9333, Val F1: 0.8889, Test F1: 0.8333\n",
      "*Epoch 102, model saved with Loss: 0.6208, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 103, model saved with Loss: 0.6693, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "*Epoch 104, model saved with Loss: 0.6354, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 105, model saved with Loss: 0.6320, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "*Epoch 106, model saved with Loss: 0.7341, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "*Epoch 107, model saved with Loss: 0.6601, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 108, model saved with Loss: 0.5141, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 109, model saved with Loss: 0.6399, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 110, model saved with Loss: 0.6663, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 111, model saved with Loss: 0.6608, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 112, model saved with Loss: 0.6809, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 113, model saved with Loss: 0.5549, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 114, model saved with Loss: 0.5847, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 115, model saved with Loss: 0.5923, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 116, model saved with Loss: 0.5091, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 117, model saved with Loss: 0.6053, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 118, model saved with Loss: 0.5367, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 119, model saved with Loss: 0.6311, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 120, model saved with Loss: 0.6534, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 121, model saved with Loss: 0.6289, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "*Epoch 122, model saved with Loss: 0.6116, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8167\n",
      "*Epoch 123, model saved with Loss: 0.6850, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 124, model saved with Loss: 0.5902, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 125, model saved with Loss: 0.5730, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 126, model saved with Loss: 0.5804, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n",
      "*Epoch 127, model saved with Loss: 0.6246, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8500\n",
      "*Epoch 128, model saved with Loss: 0.6047, Train F1: 0.9111, Val F1: 0.9111, Test F1: 0.8500\n",
      "*Epoch 129, model saved with Loss: 0.6419, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 130, model saved with Loss: 0.6099, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 148, model saved with Loss: 0.5728, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 149, model saved with Loss: 0.6435, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 150, model saved with Loss: 0.5639, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 151, model saved with Loss: 0.5489, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 152, model saved with Loss: 0.5720, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 153, model saved with Loss: 0.4961, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 158, model saved with Loss: 0.6231, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 159, model saved with Loss: 0.6018, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 160, model saved with Loss: 0.6039, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 169, model saved with Loss: 0.4814, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 170, model saved with Loss: 0.7506, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 171, model saved with Loss: 0.5500, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 172, model saved with Loss: 0.4804, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 173, model saved with Loss: 0.5531, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 174, model saved with Loss: 0.5617, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 175, model saved with Loss: 0.5610, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 183, model saved with Loss: 0.5956, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 184, model saved with Loss: 0.5677, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 185, model saved with Loss: 0.6560, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 186, model saved with Loss: 0.5854, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Epoch 195, model saved with Loss: 0.5859, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 196, model saved with Loss: 0.5129, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 197, model saved with Loss: 0.5236, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 198, model saved with Loss: 0.4968, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "Epoch 200, Loss: 0.5382, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8500\n",
      "*Epoch 201, model saved with Loss: 0.5610, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 215, model saved with Loss: 0.5734, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 216, model saved with Loss: 0.5432, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 224, model saved with Loss: 0.5115, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 225, model saved with Loss: 0.5520, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 226, model saved with Loss: 0.4776, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 227, model saved with Loss: 0.6459, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 234, model saved with Loss: 0.5227, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 235, model saved with Loss: 0.5783, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 236, model saved with Loss: 0.5106, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 237, model saved with Loss: 0.5425, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 244, model saved with Loss: 0.5531, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 245, model saved with Loss: 0.5284, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 246, model saved with Loss: 0.5639, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8500\n",
      "*Epoch 247, model saved with Loss: 0.5675, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 256, model saved with Loss: 0.5553, Train F1: 0.9111, Val F1: 0.9333, Test F1: 0.8833\n",
      "*Epoch 258, model saved with Loss: 0.5699, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.9000\n",
      "*Epoch 259, model saved with Loss: 0.4373, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 260, model saved with Loss: 0.5121, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8667\n",
      "*Epoch 261, model saved with Loss: 0.4635, Train F1: 0.9333, Val F1: 0.9333, Test F1: 0.8833\n",
      "Epoch 300, Loss: 0.4413, Train F1: 0.9333, Val F1: 0.9111, Test F1: 0.8333\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "RUNS = 1\n",
    "NUM_EPOCHS = 1000\n",
    "best_test_f1s = []\n",
    "log = True\n",
    "mode = 'learned'  # available modes ['learned', 'random', 'full']\n",
    "alternate_frequency = 0\n",
    "sample_percent = 0.20\n",
    "figure_log = False\n",
    "\n",
    "# Check if partitioning is needed\n",
    "use_metis = data.edge_index.shape[1]>=500000 # or data.num_nodes >= 20000\n",
    "\n",
    "if use_metis:\n",
    "    num_edges_per_partition = 500000\n",
    "    num_parts = int(np.ceil(data.edge_index.shape[1] / num_edges_per_partition))    \n",
    "\n",
    "#     num_parts = 1500\n",
    "#     num_edges_per_partition = int(data.edge_index.shape[1] / num_parts)\n",
    "\n",
    "    sample_size = int(num_edges_per_partition * sample_percent)\n",
    "    print(\"Using METIS with num_parts:\", num_parts, \"avg edges per partition:\", num_edges_per_partition, \"sample size:\", sample_size)\n",
    "else:\n",
    "    print(\"Graph has fewer than 10,000 nodes; don't do graph partition\")\n",
    "    sample_size = int(data.edge_index.shape[1] * sample_percent)\n",
    "\n",
    "# Partition data using METIS clustering or load the entire graph as a single batch\n",
    "if use_metis:\n",
    "    start = time.time()\n",
    "    cluster_dir = DIR + 'tmp/' + DATASET_NAME\n",
    "    if not os.path.exists(cluster_dir):\n",
    "        os.makedirs(cluster_dir)\n",
    "    \n",
    "    cluster_data = ClusterData(data, num_parts=num_parts, recursive=False, save_dir=cluster_dir, log=True)\n",
    "    print(\"METIS Partition time:\", time.time() - start)\n",
    "    cluster_loader = ClusterLoader(cluster_data, batch_size=1, shuffle=True)\n",
    "else:\n",
    "    cluster_loader = [data]\n",
    "\n",
    "# Training and evaluation loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_save_path = 'best_model.pth'\n",
    "\n",
    "train_figures = []\n",
    "\n",
    "for run in range(RUNS):\n",
    "    print(\"Run:\", run)\n",
    "    \n",
    "    train_figures = []\n",
    "\n",
    "    # Initialize model and optimizers\n",
    "    model = GNNModel(in_channels=data.x.shape[1], hidden_dim=128, out_channels=64, num_classes=dataset.num_classes).to(device)\n",
    "    optimizer_gnn = torch.optim.Adam([param for name, param in model.named_parameters() if 'gcn' in name], lr=0.01)\n",
    "    optimizer_edge_prob = torch.optim.Adam([param for name, param in model.named_parameters() if 'edge_prob_mlp' in name], lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize F1 score trackers\n",
    "    train_f1_scores = np.zeros(NUM_EPOCHS)\n",
    "    val_f1_scores = np.zeros(NUM_EPOCHS)\n",
    "    test_f1_scores = np.zeros(NUM_EPOCHS)\n",
    "    best_test_f1 = 0\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    # Training over epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Alternate training of GNN and edge probability model based on frequency\n",
    "        loss = train(\n",
    "            epoch,\n",
    "            NUM_EPOCHS,\n",
    "            model,\n",
    "            optimizer_gnn,\n",
    "            optimizer_edge_prob,\n",
    "            criterion,\n",
    "            cluster_loader,\n",
    "            device,\n",
    "            q=sample_size,\n",
    "            mode=mode,\n",
    "            alternate_frequency=alternate_frequency\n",
    "        )\n",
    "\n",
    "        # Evaluate model performance on train, validation, and test sets\n",
    "        train_f1, val_f1, test_f1 = evaluate(model, cluster_loader, device, q=int(sample_size), mode=mode)\n",
    "\n",
    "        # Store F1 scores\n",
    "        train_f1_scores[epoch] = train_f1\n",
    "        val_f1_scores[epoch] = val_f1\n",
    "        test_f1_scores[epoch] = test_f1\n",
    "\n",
    "        # Save model if validation F1 improves\n",
    "        if val_f1 >= best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            #best_test_f1 = test_f1\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            if log:\n",
    "                print(f\"*Epoch {epoch}, model saved with Loss: {loss:.4f}, Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "#          #Save model if test F1 improves\n",
    "        if test_f1 > best_test_f1:\n",
    "            best_test_f1 = test_f1\n",
    "#             torch.save(model.state_dict(), model_save_path)\n",
    "#             if log:\n",
    "#                 print(f\"*Epoch {epoch}, model saved with Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "        if log and epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, Test F1: {test_f1:.4f}')\n",
    "            \n",
    "        if figure_log:\n",
    "            train_figures.append(visualize(istest=False,show=False))\n",
    "\n",
    "    # Load the best model for evaluation\n",
    "    print(f'Best Test F1 throughout: {best_test_f1:.4f}')\n",
    "    print(f'Loading best model for final evaluation...: {best_val_f1:.4f}')\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    _, _, best_test_f1 = evaluate(model, cluster_loader, device, q=sample_size, mode=mode)\n",
    "    print(f'Best Test F1 after loading saved model: {best_test_f1:.4f}')\n",
    "    best_test_f1s.append(best_test_f1)\n",
    "\n",
    "# Report the mean and standard deviation of the best test F1 scores over runs\n",
    "print(f'Mean Std of Test F1 Score: {np.mean(best_test_f1s):.4f} +/- {np.std(best_test_f1s):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "pZIg4doOWoxl",
    "outputId": "8fd68927-460b-4c5f-d835-31a3b606afd4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the F1 scores of last run\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(train_f1_scores, label='Train F1 Score')\n",
    "plt.plot(val_f1_scores, label='Validation F1 Score')\n",
    "plt.plot(test_f1_scores, label='Test F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Scores over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uirGRnzzeXlE"
   },
   "outputs": [],
   "source": [
    "for batch in cluster_loader:\n",
    "    print(batch.num_nodes)\n",
    "    print(batch.edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Font + global sizing\n",
    "mpl.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "mpl.rcParams[\"font.serif\"] = [\"Times New Roman\", \"Times\", \"DejaVu Serif\"]\n",
    "# FIGSIZE = (5, 5)\n",
    "mpl.rcParams[\"figure.figsize\"] = FIGSIZE\n",
    "\n",
    "root_dir = Path.cwd()\n",
    "if not (root_dir / 'visualization_utils').exists() and (root_dir.parent / 'visualization_utils').exists():\n",
    "    sys.path.insert(0, str(root_dir.parent))\n",
    "\n",
    "from visualization_utils.graph import (\n",
    "    compute_graph_layout,\n",
    "    count_edges_with_different_labels,\n",
    "    plot_graph_two_class,\n",
    "    visualize_graphs_side_by_side,\n",
    ")\n",
    "from visualization_utils.embeddings import (\n",
    "    extract_embeddings,\n",
    "    reduce_embeddings_2d,\n",
    "    plot_embeddings_2d,\n",
    ")\n",
    "from visualization_utils.io import ensure_dir, save_figure\n",
    "from visualization_utils.utils import select_two_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufWVX7ANa-82",
    "outputId": "d57e6df5-8b8c-4eaf-f245-434884d5d9dc"
   },
   "outputs": [],
   "source": [
    "torch.where(data.train_mask == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helpers moved to visualization_utils (graph.py / embeddings.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WAXZjRva_nJ"
   },
   "outputs": [],
   "source": [
    "def visualize(istest=True, show=True, class_pair=None):\n",
    "    if use_metis:\n",
    "        print(\"Visualization skipped because use_metis=True\")\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        E = data.edge_index.size(1)\n",
    "\n",
    "        if mode == \"random\":\n",
    "            # Randomly sample edges (without replacement)\n",
    "            k = min(sample_size, E)\n",
    "            sampled_edge_indices = torch.randperm(E, device=data.edge_index.device)[:k]\n",
    "            sampled_edge_index = data.edge_index[:, sampled_edge_indices]\n",
    "            sampled_edge_weight = None  # optional\n",
    "        else:\n",
    "            # Your current learned sampling\n",
    "            edge_probs = model.edge_prob_mlp(data.x, data.edge_index).squeeze()\n",
    "            sampled_edge_indices, sampled_edge_weight = gumbel_softmax_sampling(\n",
    "                edge_probs,\n",
    "                data.edge_index,\n",
    "                q=sample_size,\n",
    "                istest=istest,\n",
    "            )\n",
    "            sampled_edge_index = data.edge_index[:, sampled_edge_indices]\n",
    "\n",
    "        class_pair = select_two_classes(data.y, class_pair)\n",
    "\n",
    "        fig = visualize_graphs_side_by_side(\n",
    "            data,\n",
    "            sampled_edge_index=sampled_edge_index,\n",
    "            class_pair=class_pair,\n",
    "            dataset_name=DATASET_NAME,\n",
    "            show=show,\n",
    "            figsize=(FIGSIZE[0]*2, FIGSIZE[1])\n",
    "        )\n",
    "\n",
    "        result = count_edges_with_different_labels(data, sampled_edge_index)\n",
    "\n",
    "        print(f\"Original graph: {result['original_graph']['different_label_edges']} edges with different labels \"\n",
    "              f\"out of {result['original_graph']['total_edges']} total edges \"\n",
    "              f\"(Edge homophily: {100-result['original_graph']['percentage']:.2f}%).\")\n",
    "\n",
    "        print(f\"Sampled graph: {result['sampled_graph']['different_label_edges']} edges with different labels \"\n",
    "              f\"out of {result['sampled_graph']['total_edges']} total edges \"\n",
    "              f\"(Edge homophily: {100-result['sampled_graph']['percentage']:.2f}%).\")\n",
    "\n",
    "        fig_dir = ensure_dir('KDDFigures')\n",
    "        save_figure(fig, fig_dir / str(mode+'SGSSparseGraphBoth.pdf'), dpi=300)\n",
    "\n",
    "        return fig\n",
    "\n",
    "# fig = visualize(False) #Sampled from the best\n",
    "fig = visualize(True)  #Best Sparsifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paper-ready figures (two-class highlighting)\n",
    "highlight_classes = None  # e.g., (0, 1) to fix classes; None auto-picks\n",
    "fig_dir = ensure_dir('KDDFigures')\n",
    "class_pair = select_two_classes(data.y, highlight_classes)\n",
    "istest = True\n",
    "\n",
    "if not use_metis:\n",
    "    with torch.no_grad():\n",
    "        E = data.edge_index.size(1)\n",
    "\n",
    "        if mode == \"random\":\n",
    "            # Randomly sample edges (without replacement)\n",
    "            k = min(sample_size, E)\n",
    "            sampled_edge_indices = torch.randperm(E, device=data.edge_index.device)[:k]\n",
    "            sampled_edge_index = data.edge_index[:, sampled_edge_indices]\n",
    "            sampled_edge_weight = None  # optional\n",
    "        else:\n",
    "            # Your current learned sampling\n",
    "            edge_probs = model.edge_prob_mlp(data.x, data.edge_index).squeeze()\n",
    "            sampled_edge_indices, sampled_edge_weight = gumbel_softmax_sampling(\n",
    "                edge_probs,\n",
    "                data.edge_index,\n",
    "                q=sample_size,\n",
    "                istest=istest,\n",
    "            )\n",
    "            sampled_edge_index = data.edge_index[:, sampled_edge_indices]\n",
    "\n",
    "    pos = compute_graph_layout(data, dataset_name=DATASET_NAME, seed=42)\n",
    "\n",
    "    fig_full, ax_full = plt.subplots(figsize=FIGSIZE)\n",
    "    plot_graph_two_class(\n",
    "        data,\n",
    "        ax=ax_full,\n",
    "        pos=pos,\n",
    "        class_pair=class_pair,\n",
    "        dataset_name=DATASET_NAME,\n",
    "#         title='Original dense graph',\n",
    "        node_size = 50,\n",
    "        highlight_colors=('blue', 'red'),\n",
    "    )\n",
    "    save_figure(fig_full, fig_dir / str(mode+'MoonFullGraph.pdf'), dpi=300)\n",
    "\n",
    "    fig_sparse, ax_sparse = plt.subplots(figsize=FIGSIZE)\n",
    "    plot_graph_two_class(\n",
    "        data,\n",
    "        edge_index=sampled_edge_index,\n",
    "        ax=ax_sparse,\n",
    "        pos=pos,\n",
    "        class_pair=class_pair,\n",
    "        dataset_name=DATASET_NAME,\n",
    "        #title='Learned sparse subgraph',        \n",
    "        highlight_colors=('blue', 'red'),\n",
    "    )\n",
    "    save_figure(fig_sparse, fig_dir / str(mode+'SGSSparseGraph.pdf'), dpi=300)\n",
    "\n",
    "\n",
    "    if mode == \"full\":\n",
    "        embeddings = extract_embeddings(\n",
    "            model,\n",
    "            data,\n",
    "            edge_index=data.edge_index,\n",
    "            edge_weight=None,\n",
    "        )\n",
    "    else:\n",
    "        embeddings = extract_embeddings(\n",
    "            model,\n",
    "            data,\n",
    "            edge_index=sampled_edge_index,\n",
    "            edge_weight=sampled_edge_weight,\n",
    "        )\n",
    "        \n",
    "    emb_2d = reduce_embeddings_2d(embeddings, method='pca', random_state=42)\n",
    "    fig_emb, ax_emb = plt.subplots(figsize=FIGSIZE)\n",
    "    plot_embeddings_2d(\n",
    "        emb_2d,\n",
    "        data.y,\n",
    "        class_pair=class_pair,\n",
    "        ax=ax_emb,        \n",
    "#         title='Learned embeddings (PCA)',\n",
    "        highlight_colors=('blue', 'red'),\n",
    "    )\n",
    "    save_figure(fig_emb, fig_dir / str(mode+'SGSSparseEmbedding.pdf'), dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Skipping visualization because use_metis=True (graph too large).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (My py311cu117pyg200 Kernel)",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
