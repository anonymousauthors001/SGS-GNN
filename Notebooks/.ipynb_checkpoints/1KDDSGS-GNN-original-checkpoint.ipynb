{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZGUfQ1TB86e",
    "outputId": "fda95a5b-cff2-45a7-f5b2-c4207aae9666"
   },
   "outputs": [],
   "source": [
    "# %pip install torch_geometric\n",
    "# %pip install ogb\n",
    "# %pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZtT5Sw_8zb8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid, Reddit\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch_geometric.datasets import WikipediaNetwork\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.typing import Adj, SparseTensor\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "from torch_geometric.utils import scatter\n",
    "from torch_geometric.datasets import Reddit, Reddit2, Flickr, Yelp, AmazonProducts, PPI,  OGB_MAG,  FakeDataset, Amazon,Coauthor,HeterophilousGraphDataset, CitationFull\n",
    "from torch_geometric.utils import homophily\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "def fix_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuE5unEi0s3b",
    "outputId": "82289b7d-95d8-429c-f0bb-4aa797f3de7f"
   },
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import LINKXDataset\n",
    "# dataset_name = 'Roman-empire'\n",
    "# dataset = HeterophilousGraphDataset('/tmp/Cora', DATASET_NAME)\n",
    "# DATASET_NAME = dataset_name\n",
    "# data = dataset[0]\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "ZzPGehcRifbP",
    "outputId": "b26f08a1-e66c-4e3e-9aba-48ff329feb69"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import KarateClub, Reddit, Reddit2, Flickr, Yelp, AmazonProducts, PPI,  OGB_MAG,  FakeDataset, Amazon,Coauthor,HeterophilousGraphDataset,LINKXDataset\n",
    "\n",
    "def load_dataset(dataset_name):\n",
    "    if dataset_name == \"karate\":\n",
    "        dataset = KarateClub()\n",
    "    elif dataset_name == \"moon\":\n",
    "        from ipynb.fs.full.Moon import MoonDataset\n",
    "        dataset = MoonDataset(n_samples=150, degree=4, train=0.3, h = 0.2)\n",
    "    elif dataset_name == \"SmallCora\":\n",
    "        dataset = Planetoid(root=DIR+'/tmp/Cora', name='Cora')\n",
    "    elif dataset_name in [\"Cora\", \"Cora_ML\" \"CiteSeer\", \"DBLP\", \"PubMed\"]:\n",
    "        dataset = CitationFull(root=DIR+'/tmp/Citation/'+dataset_name, name=dataset_name)\n",
    "    elif dataset_name == 'Amazon-ratings':\n",
    "        dataset = HeterophilousGraphDataset(root=DIR+'/tmp/amazon_ratings', name = dataset_name)\n",
    "    elif dataset_name == 'Roman-empire':\n",
    "        from torch_geometric.datasets import LINKXDataset\n",
    "        dataset = LINKXDataset(DIR+'/tmp/Roman_empire', dataset_name)\n",
    "    elif dataset_name == \"Reddit\":\n",
    "        dataset = Reddit(root=DIR+'/tmp/Reddit')\n",
    "    elif dataset_name == 'penn94':\n",
    "        from torch_geometric.datasets import LINKXDataset\n",
    "        dataset = LINKXDataset(root=DIR+'/tmp/LINKX', name=dataset_name)\n",
    "    elif dataset_name == 'wiki':\n",
    "        from torch_geometric.datasets import WikiCS\n",
    "        dataset = WikiCS(root=DIR+'/tmp/WikiCS')\n",
    "    elif dataset_name == 'Photo':\n",
    "        dataset = Amazon(root=DIR+'/tmp/Photo', name='Photo')\n",
    "    elif dataset_name == 'CiteSeer':\n",
    "        dataset = Planetoid(root=DIR+'/tmp/CiteSeer', name=DATASET_NAME)\n",
    "    elif dataset_name == 'CS':\n",
    "        dataset = Coauthor(root=DIR+'/tmp/CS', name = DATASET_NAME)\n",
    "    elif dataset_name == 'Physics':\n",
    "        dataset = Coauthor(root=DIR+'/tmp/Physics', name = DATASET_NAME)\n",
    "    elif dataset_name == 'Minesweeper':\n",
    "        dataset   = HeterophilousGraphDataset(root=DIR+'/tmp/Mine',name=dataset_name)\n",
    "    elif dataset_name == 'pokec':\n",
    "        print(\"HELLOWROLD \")\n",
    "    elif dataset_name == 'ogbn-proteins':\n",
    "        dataset = PygNodePropPredDataset(name='ogbn-proteins', root=DIR+'/tmp/ogbn-proteins')\n",
    "        data = dataset[0]\n",
    "        data.node_species = None\n",
    "        data.y = data.y.to(torch.float)\n",
    "\n",
    "        # Initialize features of nodes by aggregating edge features.\n",
    "        row, col = data.edge_index\n",
    "        data.x = scatter(data.edge_attr, col, dim_size=data.num_nodes, reduce='sum')\n",
    "        labels = data.y.argmax(dim=1)\n",
    "        data.y = labels\n",
    "        return dataset, data\n",
    "    elif dataset_name == 'squirrel':\n",
    "        dataset = WikipediaNetwork(root=DIR+'/tmp/squirrel', name='Squirrel')\n",
    "    elif dataset_name == 'AmazonProducts':\n",
    "        dataset = AmazonProducts(root=DIR+'/tmp/AmazonProducts')\n",
    "    else:\n",
    "        dataset = Planetoid(root=DIR+'/tmp/Cora', name='Cora')\n",
    "\n",
    "    print(dataset)\n",
    "    data = dataset[0]\n",
    "    print(data)\n",
    "    return dataset, data\n",
    "\n",
    "# Example usage\n",
    "dataset_name = \"moon\"\n",
    "DATASET_NAME = dataset_name\n",
    "dataset, data = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUx2t-uxjmV5"
   },
   "outputs": [],
   "source": [
    "# dataset_name = 'Roman-empire'\n",
    "# DATASET_NAME = dataset_name\n",
    "# dataset   = HeterophilousGraphDataset(root='/tmp/roman',name=dataset_name)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gapSL8SlRus",
    "outputId": "a6d02cfc-031b-4d64-9018-3fe9e5648cb2"
   },
   "outputs": [],
   "source": [
    "dataset.num_classes\n",
    "He = homophily(data.edge_index, data.y, method='edge')\n",
    "print(\"Edge Homophily: \", He)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzVprESJYmid"
   },
   "outputs": [],
   "source": [
    "def train_val_test_mask(data, train=0.4, val=0.3, test=0.3, random_state=False):\n",
    "\n",
    "    if isinstance(data.x, SparseTensor):\n",
    "        N = data.x.size(0)\n",
    "        data.num_nodes = N\n",
    "    else:\n",
    "        N = data.x.shape[0]\n",
    "\n",
    "    indexs = list(range(N))\n",
    "\n",
    "    if random_state:\n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test))\n",
    "    else:\n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True\n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"val_mask\" not in data.__dict__['_store']:    \n",
    "    data = train_val_test_mask(data, train=0.2, val=0.4, test=0.4)\n",
    "\n",
    "if len(data.train_mask.shape) > 1:\n",
    "    split_index = 2\n",
    "    data.train_mask = data.train_mask[:,split_index]\n",
    "    data.val_mask = data.val_mask[:,split_index]\n",
    "    data.test_mask = data.test_mask[:,split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP for edge probability with dropout\n",
    "class EdgeProbMLP(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, dropout_prob=0.5):\n",
    "        super(EdgeProbMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(2 * in_channels, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, node_features, edge_index):\n",
    "        edge_features = torch.cat([node_features[edge_index[0]], node_features[edge_index[1]]], dim=1)\n",
    "        x = F.relu(self.fc1(edge_features))\n",
    "        #x = self.dropout(x)\n",
    "        prob = torch.sigmoid(self.fc2(x))\n",
    "        return prob\n",
    "\n",
    "# Define the overall model with Edge Probabilities and GCNConv (GNN) with dropout\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels, num_classes, dropout_prob=0.5):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.edge_prob_mlp = EdgeProbMLP(in_channels, hidden_dim, dropout_prob)\n",
    "        self.gcn1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.gcn2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data, edge_index, edge_weight=None):\n",
    "        x = F.relu(self.gcn1(data.x, edge_index, edge_weight))\n",
    "        #x = self.dropout(x)\n",
    "        out = self.gcn2(x, edge_index, edge_weight)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGg6LqrCBHVZ"
   },
   "outputs": [],
   "source": [
    "# Gumbel-Softmax trick for edge sampling\n",
    "def gumbel_softmax_sampling(edge_probs, edge_index, q=500, temperature=0.5, log=False, istest=False):\n",
    "        \n",
    "    if istest:\n",
    "        samples = F.softmax(edge_probs/temperature, dim=-1)\n",
    "        top_k_values, sampled_edges = torch.topk(edge_probs, q, dim=-1, largest=True, sorted=True)\n",
    "    else:\n",
    "        samples = F.softmax(edge_probs/temperature, dim=-1)\n",
    "        sampled_edges = torch.multinomial(samples, q, replacement=False) #nothing\n",
    "\n",
    "#     gumbels = -torch.empty_like(edge_probs).exponential_().log()\n",
    "#     gumbels = (gumbels - gumbels.min()) / (gumbels.max() - gumbels.min())\n",
    "#     gumbels = gumbels/10.0\n",
    "\n",
    "#     logits = (edge_probs + gumbels) / temperature    \n",
    "#     samples = F.softmax(logits, dim=-1)\n",
    "\n",
    "#     top_k_values, sampled_edges = torch.topk(samples, q, dim=-1, largest=True, sorted=True) # 50       \n",
    "#     sampled_edges = torch.multinomial(samples, q, replacement=False)\n",
    "#     top_k_values, sampled_edges = torch.topk(edge_probs, q, dim=-1, largest=True, sorted=True)    \n",
    "    \n",
    "    one_hot = torch.zeros_like(samples)\n",
    "    one_hot.scatter_(0, sampled_edges, 1.0)     \n",
    "    \n",
    "    indexs = (one_hot - samples).detach() + samples\n",
    "    \n",
    "#     if log:\n",
    "#         print(\"indexs before: \",indexs)\n",
    "    \n",
    "    edge_probs = edge_probs*indexs\n",
    "    \n",
    "    indexs = indexs.bool()\n",
    "    \n",
    "    \n",
    "#     if log:        \n",
    "#         print(\"edge_probs: \",edge_probs)\n",
    "# #         print(\"gumbels: \", gumbels)\n",
    "# #         print(\"logits: \",logits)\n",
    "#         print(\"samples: \",samples)\n",
    "#         print(\"sampled_edges: \",sampled_edges)\n",
    "#         print(\"One hot: \", one_hot)\n",
    "#         print(\"Indexs: \",indexs)\n",
    "#         print(\"edge_probs[indexs]:\", edge_probs[indexs])\n",
    "\n",
    "    \n",
    "    return indexs, edge_probs[indexs]\n",
    "\n",
    "\n",
    "# Random edge sampling\n",
    "def random_edge_sampling(edge_index, q):\n",
    "    num_edges = edge_index.shape[1]\n",
    "    sampled_indices = torch.randperm(num_edges)[:q]\n",
    "    sampled_edge_index = edge_index[:, sampled_indices]\n",
    "    return sampled_edge_index\n",
    "\n",
    "# F1 score calculation helper\n",
    "def calculate_f1(logits, labels, mask):\n",
    "    preds = logits[mask].argmax(dim=1)\n",
    "    f1 = f1_score(labels[mask].cpu(), preds.cpu(), average='micro')\n",
    "    return f1\n",
    "\n",
    "# Training function with alternating updates\n",
    "def train(epoch, max_epoch, model, optimizer_gnn, optimizer_edge_prob, criterion, cluster_loader, device, q=500, mode='learned', alternate_frequency=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    iteration = 0\n",
    "\n",
    "    for batch in cluster_loader:\n",
    "        if not batch.train_mask.any():\n",
    "            continue\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        if alternate_frequency == 0:\n",
    "            update_gnn = True\n",
    "            update_edge_prob = True\n",
    "        else:            \n",
    "            # Decide if we are updating GNN or edge probability model\n",
    "            update_gnn = iteration % alternate_frequency == 0\n",
    "            update_edge_prob = not update_gnn\n",
    "\n",
    "\n",
    "        # Zero gradients selectively\n",
    "        if update_gnn:\n",
    "            optimizer_gnn.zero_grad()\n",
    "        if update_edge_prob:\n",
    "            optimizer_edge_prob.zero_grad()\n",
    "\n",
    "        # Select edges based on mode\n",
    "        if mode == 'learned':\n",
    "            if batch.edge_index.shape[1] > q:\n",
    "                edge_probs = model.edge_prob_mlp(batch.x, batch.edge_index).squeeze()                    \n",
    "                temperature = 2.0 - (epoch * (2 - 0.5) / max_epoch)\n",
    "                #temperature = 10\n",
    "                #print(temperature)                \n",
    "                sampled_edge_indices, sampled_edge_weight = gumbel_softmax_sampling(edge_probs, batch.edge_index, q=q, temperature=temperature, log=(epoch==max_epoch-1))\n",
    "                sampled_edge_index = batch.edge_index[:, sampled_edge_indices]            \n",
    "                out = model(batch, sampled_edge_index, sampled_edge_weight) \n",
    "                #out = model(batch, sampled_edge_index, None)                                        \n",
    "            else:\n",
    "                out = model(batch, batch.edge_index)\n",
    "        elif mode == 'random':\n",
    "            if batch.edge_index.shape[1] > q:\n",
    "                sampled_edge_index = random_edge_sampling(batch.edge_index, q=q)\n",
    "                out = model(batch, sampled_edge_index)\n",
    "            else:\n",
    "                out = model(batch, batch.edge_index)\n",
    "        elif mode == 'full':\n",
    "            out = model(batch, batch.edge_index)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'learned', 'random', or 'full'.\")\n",
    "\n",
    "        # Compute node classification loss\n",
    "        loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        \n",
    "        if mode == 'learned' and batch.edge_index.shape[1] > q:        \n",
    "            \n",
    "            ########---regularizer 2\n",
    "            \n",
    "            train_indices = torch.nonzero(batch.train_mask).squeeze()            \n",
    "            num_nodes = train_indices.size(0)\n",
    "            num_edges = min(num_nodes*num_nodes,q)  # Adjust the number of edges as needed\n",
    "\n",
    "            # Generate random edges\n",
    "            src = torch.randint(0, num_nodes, (num_edges,))\n",
    "            dst = torch.randint(0, num_nodes, (num_edges,))\n",
    "\n",
    "            # Map to actual node indices\n",
    "            src_nodes = train_indices[src]\n",
    "            dst_nodes = train_indices[dst]\n",
    "\n",
    "            # Create edge_index tensor\n",
    "            reg_edge_index = torch.stack([src_nodes, dst_nodes], dim=0).to(device)\n",
    "            reg_edge_label = (batch.y[src_nodes] == batch.y[dst_nodes]).float().to(device)\n",
    "            \n",
    "            reg_edge_probs = model.edge_prob_mlp(batch.x, reg_edge_index).squeeze()\n",
    "            \n",
    "            loss4 = F.binary_cross_entropy(reg_edge_probs, reg_edge_label)\n",
    "\n",
    "            #print(reg_edge_index)\n",
    "            #print(reg_edge_label)\n",
    "            \n",
    "            loss = loss+loss4\n",
    "            \n",
    "            \n",
    "#             #######---regularizer 1\n",
    "#             edge_labels = torch.full((batch.edge_index.size(1),), -1, dtype=torch.long, device=device)\n",
    "#             train_indices = torch.nonzero(batch.train_mask).squeeze()\n",
    "#             train_edge_mask = torch.isin(batch.edge_index[0], train_indices) & torch.isin(batch.edge_index[1], train_indices)\n",
    "\n",
    "#             # Assign labels based on the class of the endpoints\n",
    "#             same_class_mask = batch.y[batch.edge_index[0]] == batch.y[batch.edge_index[1]]\n",
    "#             edge_labels[train_edge_mask & same_class_mask] = 1\n",
    "#             edge_labels[train_edge_mask & ~same_class_mask] = 0\n",
    "\n",
    "#             # Filter out the edges with labels -1\n",
    "#             valid_edge_mask = edge_labels != -1\n",
    "#             valid_edge_probs = edge_probs[valid_edge_mask]\n",
    "#             valid_edge_labels = edge_labels[valid_edge_mask]\n",
    "            \n",
    "#             #print(torch.sum(valid_edge_labels))\n",
    "#             if torch.sum(valid_edge_labels).item() > 1:            \n",
    "#                 loss2 = F.binary_cross_entropy(valid_edge_probs, valid_edge_labels.float().to(device))\n",
    "#             else:\n",
    "#                 loss2 = 0\n",
    "                \n",
    "#             loss = loss+loss2\n",
    "                        \n",
    "#             #### ----regularizer 3\n",
    "#             neg_q = int(torch.sum(valid_edge_labels).item())            \n",
    "#             neg_q = q\n",
    "#             neg_edge_index = negative_sampling(batch.edge_index, num_nodes=batch.num_nodes, num_neg_samples=neg_q)\n",
    "#             neg_edge_probs = model.edge_prob_mlp(batch.x, neg_edge_index).squeeze()\n",
    "#             neg_edge_labels = torch.zeros(neg_q, dtype=torch.float, device=batch.edge_index.device)            \n",
    "#             loss3 = F.binary_cross_entropy(neg_edge_probs, neg_edge_labels)\n",
    "#             loss = loss+loss3\n",
    "#             \n",
    "            \n",
    "#             #### ------Total Loss            \n",
    "#             print(\"Loss1: \",loss.item(), \" Loss2: \",loss2,\" Loss3: \",loss3)            \n",
    "#             alpha1 = 1.0 \n",
    "#             alpha2 = 1.0 \n",
    "#             loss = loss + alpha1*loss2 + alpha2*loss3                        \n",
    "#             print(\"Total loss: \",loss.item())\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters selectively\n",
    "        if update_gnn:\n",
    "            optimizer_gnn.step()\n",
    "        if update_edge_prob:\n",
    "            optimizer_edge_prob.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        iteration += 1\n",
    "\n",
    "    return total_loss / len(cluster_loader)\n",
    "\n",
    "# Evaluation function remains unchanged\n",
    "def evaluate(model, cluster_loader, device, q=500, mode=None):\n",
    "    model.eval()\n",
    "    total_train_f1, total_val_f1, total_test_f1 = 0, 0, 0\n",
    "    num_train_examples, num_val_examples, num_test_examples = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in cluster_loader:\n",
    "            batch = batch.to(device)\n",
    "            if mode == 'learned':\n",
    "                if batch.edge_index.shape[1] > q:\n",
    "                    edge_probs = model.edge_prob_mlp(batch.x, batch.edge_index).squeeze()                    \n",
    "                    sampled_edge_indices, sampled_edge_weight = gumbel_softmax_sampling(edge_probs, batch.edge_index, q=q, istest=True)\n",
    "                    sampled_edge_index = batch.edge_index[:, sampled_edge_indices]\n",
    "                    out =model(batch, sampled_edge_index, sampled_edge_weight)                                        \n",
    "                    #out = model(batch, sampled_edge_index, None)                                        \n",
    "                else:\n",
    "                    out = model(batch, batch.edge_index)\n",
    "            elif mode == 'random':\n",
    "                if batch.edge_index.shape[1] > q:\n",
    "                    sampled_edge_index = random_edge_sampling(batch.edge_index, q=q)\n",
    "                    out = model(batch, sampled_edge_index)\n",
    "                else:\n",
    "                    out = model(batch, batch.edge_index)\n",
    "            elif mode == 'full':\n",
    "                out = model(batch, batch.edge_index)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode. Choose 'learned', 'random', or 'full'.\")\n",
    "\n",
    "            # Calculate F1 scores for train, val, and test sets if masks are present\n",
    "            if batch.train_mask.any():\n",
    "                train_f1 = calculate_f1(out, batch.y, batch.train_mask)\n",
    "                total_train_f1 += train_f1 * batch.train_mask.sum().item()\n",
    "                num_train_examples += batch.train_mask.sum().item()\n",
    "\n",
    "            if batch.val_mask.any():\n",
    "                val_f1 = calculate_f1(out, batch.y, batch.val_mask)\n",
    "                total_val_f1 += val_f1 * batch.val_mask.sum().item()\n",
    "                num_val_examples += batch.val_mask.sum().item()\n",
    "\n",
    "            if batch.test_mask.any():\n",
    "                test_f1 = calculate_f1(out, batch.y, batch.test_mask)\n",
    "                total_test_f1 += test_f1 * batch.test_mask.sum().item()\n",
    "                num_test_examples += batch.test_mask.sum().item()\n",
    "\n",
    "    avg_train_f1 = total_train_f1 / num_train_examples if num_train_examples > 0 else 0\n",
    "    avg_val_f1 = total_val_f1 / num_val_examples if num_val_examples > 0 else 0\n",
    "    avg_test_f1 = total_test_f1 / num_test_examples if num_test_examples > 0 else 0\n",
    "\n",
    "    return avg_train_f1, avg_val_f1, avg_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"edge_weight\" in data.__dict__['_store']:\n",
    "    print(\"Edge weights given\")\n",
    "else:\n",
    "    print(\"No edge weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# def adj_feature(data):    \n",
    "#     adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "#     edges = data.edge_index.t()\n",
    "#     adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "#     adj_mat[edges[:,1], edges[:,0]] = 1\n",
    "    \n",
    "# #     return adj_mat\n",
    "    \n",
    "# #     n_components = data.x.shape[1]\n",
    "#     n_components = min(256, data.x.shape[1], data.num_nodes)\n",
    "\n",
    "#     svd = TruncatedSVD(n_components=n_components)\n",
    "#     x = svd.fit_transform(adj_mat)\n",
    "    \n",
    "#     x = torch.Tensor(x)\n",
    "#     x.shape    \n",
    "    \n",
    "#     return x\n",
    "\n",
    "# data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "\n",
    "# # data.x = adj_feature(data)\n",
    "# # print(data.x.shape)\n",
    "# # data.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQoF33fWMAUd"
   },
   "source": [
    "Main for multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTWoJucaYyS0",
    "outputId": "14020a06-a242-4387-a0a2-6bba52c79091"
   },
   "outputs": [],
   "source": [
    "# data = train_val_test_mask(data, train=0.5, val=0.25, test=0.25)\n",
    "print(sum(data.train_mask))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdXts5zdPRso",
    "outputId": "c160ec6b-5dbd-4ae1-fb06-7216c2f9af62",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RUNS = 1\n",
    "NUM_EPOCHS = 1000\n",
    "best_test_f1s = []\n",
    "log = True\n",
    "mode = 'learned'  # available modes ['learned', 'random', 'full']\n",
    "alternate_frequency = 0\n",
    "sample_percent = 0.20\n",
    "figure_log = False\n",
    "\n",
    "# Check if partitioning is needed\n",
    "use_metis = data.edge_index.shape[1]>=500000 # or data.num_nodes >= 20000\n",
    "\n",
    "if use_metis:\n",
    "    num_edges_per_partition = 500000\n",
    "    num_parts = int(np.ceil(data.edge_index.shape[1] / num_edges_per_partition))    \n",
    "\n",
    "#     num_parts = 1500\n",
    "#     num_edges_per_partition = int(data.edge_index.shape[1] / num_parts)\n",
    "\n",
    "    sample_size = int(num_edges_per_partition * sample_percent)\n",
    "    print(\"Using METIS with num_parts:\", num_parts, \"avg edges per partition:\", num_edges_per_partition, \"sample size:\", sample_size)\n",
    "else:\n",
    "    print(\"Graph has fewer than 10,000 nodes; don't do graph partition\")\n",
    "    sample_size = int(data.edge_index.shape[1] * sample_percent)\n",
    "\n",
    "# Partition data using METIS clustering or load the entire graph as a single batch\n",
    "if use_metis:\n",
    "    start = time.time()\n",
    "    cluster_dir = DIR + 'tmp/' + DATASET_NAME\n",
    "    if not os.path.exists(cluster_dir):\n",
    "        os.makedirs(cluster_dir)\n",
    "    \n",
    "    cluster_data = ClusterData(data, num_parts=num_parts, recursive=False, save_dir=cluster_dir, log=True)\n",
    "    print(\"METIS Partition time:\", time.time() - start)\n",
    "    cluster_loader = ClusterLoader(cluster_data, batch_size=1, shuffle=True)\n",
    "else:\n",
    "    cluster_loader = [data]\n",
    "\n",
    "# Training and evaluation loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_save_path = 'best_model.pth'\n",
    "\n",
    "train_figures = []\n",
    "\n",
    "for run in range(RUNS):\n",
    "    print(\"Run:\", run)\n",
    "    \n",
    "    train_figures = []\n",
    "\n",
    "    # Initialize model and optimizers\n",
    "    model = GNNModel(in_channels=data.x.shape[1], hidden_dim=128, out_channels=64, num_classes=dataset.num_classes).to(device)\n",
    "    optimizer_gnn = torch.optim.Adam([param for name, param in model.named_parameters() if 'gcn' in name], lr=0.01)\n",
    "    optimizer_edge_prob = torch.optim.Adam([param for name, param in model.named_parameters() if 'edge_prob_mlp' in name], lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize F1 score trackers\n",
    "    train_f1_scores = np.zeros(NUM_EPOCHS)\n",
    "    val_f1_scores = np.zeros(NUM_EPOCHS)\n",
    "    test_f1_scores = np.zeros(NUM_EPOCHS)\n",
    "    best_test_f1 = 0\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    # Training over epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Alternate training of GNN and edge probability model based on frequency\n",
    "        loss = train(\n",
    "            epoch,\n",
    "            NUM_EPOCHS,\n",
    "            model,\n",
    "            optimizer_gnn,\n",
    "            optimizer_edge_prob,\n",
    "            criterion,\n",
    "            cluster_loader,\n",
    "            device,\n",
    "            q=sample_size,\n",
    "            mode=mode,\n",
    "            alternate_frequency=alternate_frequency\n",
    "        )\n",
    "\n",
    "        # Evaluate model performance on train, validation, and test sets\n",
    "        train_f1, val_f1, test_f1 = evaluate(model, cluster_loader, device, q=int(sample_size), mode=mode)\n",
    "\n",
    "        # Store F1 scores\n",
    "        train_f1_scores[epoch] = train_f1\n",
    "        val_f1_scores[epoch] = val_f1\n",
    "        test_f1_scores[epoch] = test_f1\n",
    "\n",
    "        # Save model if validation F1 improves\n",
    "        if val_f1 >= best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            #best_test_f1 = test_f1\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            if log:\n",
    "                print(f\"*Epoch {epoch}, model saved with Loss: {loss:.4f}, Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "#          #Save model if test F1 improves\n",
    "        if test_f1 > best_test_f1:\n",
    "            best_test_f1 = test_f1\n",
    "#             torch.save(model.state_dict(), model_save_path)\n",
    "#             if log:\n",
    "#                 print(f\"*Epoch {epoch}, model saved with Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "        if log and epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, Test F1: {test_f1:.4f}')\n",
    "            \n",
    "        if figure_log:\n",
    "            train_figures.append(visualize(istest=False,show=False))\n",
    "\n",
    "    # Load the best model for evaluation\n",
    "    print(f'Best Test F1 throughout: {best_test_f1:.4f}')\n",
    "    print(f'Loading best model for final evaluation...: {best_val_f1:.4f}')\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    _, _, best_test_f1 = evaluate(model, cluster_loader, device, q=sample_size, mode=mode)\n",
    "    print(f'Best Test F1 after loading saved model: {best_test_f1:.4f}')\n",
    "    best_test_f1s.append(best_test_f1)\n",
    "\n",
    "# Report the mean and standard deviation of the best test F1 scores over runs\n",
    "print(f'Mean Std of Test F1 Score: {np.mean(best_test_f1s):.4f} +/- {np.std(best_test_f1s):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "pZIg4doOWoxl",
    "outputId": "8fd68927-460b-4c5f-d835-31a3b606afd4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the F1 scores of last run\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_f1_scores, label='Train F1 Score')\n",
    "plt.plot(val_f1_scores, label='Validation F1 Score')\n",
    "plt.plot(test_f1_scores, label='Test F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Scores over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uirGRnzzeXlE"
   },
   "outputs": [],
   "source": [
    "for batch in cluster_loader:\n",
    "    print(batch.num_nodes)\n",
    "    print(batch.edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import matplotlib.colors as mcolors\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufWVX7ANa-82",
    "outputId": "d57e6df5-8b8c-4eaf-f245-434884d5d9dc"
   },
   "outputs": [],
   "source": [
    "torch.where(data.train_mask == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_edges_with_different_labels(data, sampled_edge_index):\n",
    "    \"\"\"\n",
    "    Counts edges with different labels at endpoints in both the original and sampled graphs.\n",
    "    \n",
    "    Args:\n",
    "        data: Data object containing the original graph information.\n",
    "        sampled_edge_index: Edge index of the sampled subgraph.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with counts and percentages of edges with different labels.\n",
    "    \"\"\"\n",
    "    # Original graph edges\n",
    "    original_edges = data.edge_index\n",
    "    num_original_edges = original_edges.size(1)\n",
    "    \n",
    "    # Count edges with different labels in the original graph\n",
    "    different_label_edges_original = 0\n",
    "    for i in range(num_original_edges):\n",
    "        node1, node2 = original_edges[:, i]\n",
    "        if data.y[node1].item() != data.y[node2].item():\n",
    "            different_label_edges_original += 1\n",
    "    \n",
    "    # Sampled graph edges\n",
    "    sampled_edges = sampled_edge_index\n",
    "    num_sampled_edges = sampled_edges.size(1)\n",
    "\n",
    "    # Count edges with different labels in the sampled graph\n",
    "    different_label_edges_sampled = 0\n",
    "    for i in range(num_sampled_edges):\n",
    "        node1, node2 = sampled_edges[:, i]\n",
    "        if data.y[node1].item() != data.y[node2].item():\n",
    "            different_label_edges_sampled += 1\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentage_original = (different_label_edges_original / num_original_edges) * 100 if num_original_edges > 0 else 0\n",
    "    percentage_sampled = (different_label_edges_sampled / num_sampled_edges) * 100 if num_sampled_edges > 0 else 0\n",
    "\n",
    "    # Prepare the result\n",
    "    result = {\n",
    "        \"original_graph\": {\n",
    "            \"different_label_edges\": different_label_edges_original,\n",
    "            \"total_edges\": num_original_edges,\n",
    "            \"percentage\": percentage_original\n",
    "        },\n",
    "        \"sampled_graph\": {\n",
    "            \"different_label_edges\": different_label_edges_sampled,\n",
    "            \"total_edges\": num_sampled_edges,\n",
    "            \"percentage\": percentage_sampled\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WAXZjRva_nJ"
   },
   "outputs": [],
   "source": [
    "def plot_full_graph_with_train_nodes(data, train_mask, ax=None):\n",
    "    \"\"\"\n",
    "    Plots the full graph with training nodes highlighted and colored by their labels.\n",
    "    Args:\n",
    "        data: Data object containing edge_index, node labels, and other features.\n",
    "        train_mask: Boolean tensor indicating training nodes.\n",
    "        ax: Matplotlib axis on which to plot (for side-by-side plotting).\n",
    "    \"\"\"\n",
    "    # Convert PyG data to NetworkX graph for visualization\n",
    "    G = to_networkx(data)\n",
    "    \n",
    "    # Generate color map for each label\n",
    "    unique_labels = torch.unique(data.y[train_mask]).tolist()\n",
    "    color_map = {label: color for label, color in zip(unique_labels, mcolors.TABLEAU_COLORS)}\n",
    "\n",
    "    # Set node colors based on whether they are a training node and their label\n",
    "    node_colors = [\n",
    "        color_map[data.y[i].item()] if train_mask[i].item() else 'gray' for i in range(data.num_nodes)\n",
    "    ]\n",
    "    \n",
    "    pos = None\n",
    "    \n",
    "    if DATASET_NAME == 'moon':\n",
    "        pos = {i: (data.x[i][0].item(), data.x[i][1].item()) for i in range(len(data.x))}\n",
    "    elif pos is None:\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "    # Plot the full graph with training nodes highlighted by label color\n",
    "    nx.draw(G, pos, node_color=node_colors, ax=ax, with_labels=False, node_size=100, edge_color=\"lightblue\")\n",
    "    \n",
    "#     # Add label above each node showing the label value\n",
    "#     for node, p in pos.items():\n",
    "#         label = data.y[node].item()\n",
    "#         ax.text(p[0], p[1] + 0.05, str(label), fontsize=8, ha='center', va='center')\n",
    "    \n",
    "    ax.set_title(\"Original dense graph\",fontsize=20)\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    return pos\n",
    "\n",
    "\n",
    "def plot_sampled_subgraph(data, sampled_edge_index, train_mask, ax=None, pos=None):\n",
    "    \"\"\"\n",
    "    Plots the sampled subgraph based on the edge probability model with training nodes highlighted by label.\n",
    "    Args:\n",
    "        data: Data object containing node information.\n",
    "        sampled_edge_index: Edge index of the sampled subgraph.\n",
    "        train_mask: Boolean tensor indicating training nodes.\n",
    "        ax: Matplotlib axis on which to plot (for side-by-side plotting).\n",
    "    \"\"\"\n",
    "    # Convert sampled edges to NetworkX graph\n",
    "    G_sampled = to_networkx(Data(x = data.x, y = data.y, edge_index = sampled_edge_index))\n",
    "    \n",
    "    # Use same color map as the full graph\n",
    "    unique_labels = torch.unique(data.y[train_mask]).tolist()\n",
    "    color_map = {label: color for label, color in zip(unique_labels, mcolors.TABLEAU_COLORS)}\n",
    "\n",
    "    # Use same layout as the full graph for consistent positioning\n",
    "    if pos is None:\n",
    "        pos = nx.spring_layout(G_sampled, seed=42)\n",
    "    \n",
    "    # Set node colors based on training status and label for nodes in the sampled graph\n",
    "    node_colors = [\n",
    "        color_map[data.y[i].item()] if train_mask[i].item() and i in G_sampled.nodes else 'gray'\n",
    "        for i in G_sampled.nodes\n",
    "    ]\n",
    "\n",
    "    # Plot the sampled subgraph with node labels\n",
    "    nx.draw(G_sampled, pos, node_color=node_colors, ax=ax, with_labels=False, node_size=100, edge_color=\"lightblue\")\n",
    "    \n",
    "#     # Add label above each node showing the label value\n",
    "#     for node, p in pos.items():\n",
    "#         label = data.y[node].item()\n",
    "#         ax.text(p[0], p[1] + 0.05, str(label), fontsize=8, ha='center', va='center')\n",
    "    \n",
    "    ax.set_title(\"Learned sparse subgraph with 20% edges\", fontsize=20)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def visualize_graphs_side_by_side(data, train_mask, sampled_edge_index, show=True):\n",
    "    \"\"\"\n",
    "    Visualizes the full graph with training nodes and the sampled subgraph side by side, with training nodes colored by label.\n",
    "    Args:\n",
    "        data: Data object containing the full graph information.\n",
    "        train_mask: Boolean tensor indicating training nodes.\n",
    "        sampled_edge_index: Edge index of the sampled subgraph.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    # Plot full graph with training nodes highlighted by label\n",
    "    pos1 = plot_full_graph_with_train_nodes(data, train_mask, ax=ax1)\n",
    "\n",
    "    # Plot sampled subgraph with training nodes highlighted by label, using the same layout\n",
    "    plot_sampled_subgraph(data, sampled_edge_index, train_mask, ax=ax2, pos=pos1)\n",
    "    \n",
    "    if show==False:\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming `train_mask` is a boolean mask tensor, and `sampled_edge_index` is the edge index after sampling\n",
    "# visualize_graphs_side_by_side(data, train_mask=data.train_mask, sampled_edge_index=sampled_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize(istest=True, show=True):\n",
    "    if use_metis == False:\n",
    "        with torch.no_grad():\n",
    "            edge_probs = model.edge_prob_mlp(data.x, data.edge_index).squeeze()\n",
    "            sampled_edge_indices, sampled_edge_weight = gumbel_softmax_sampling(\n",
    "                edge_probs,\n",
    "                data.edge_index,\n",
    "                q=sample_size, istest=istest)\n",
    "\n",
    "            sampled_edge_index = data.edge_index[:, sampled_edge_indices]\n",
    "            fig = visualize_graphs_side_by_side(data, train_mask=data.train_mask, sampled_edge_index=sampled_edge_index, show=show)\n",
    "\n",
    "            # Example Usage\n",
    "            # Assuming `data` is your original graph and `sampled_edge_index` is your sampled edge index\n",
    "            result = count_edges_with_different_labels(data, sampled_edge_index)\n",
    "\n",
    "#             print(\"\\% of heterophily:\")\n",
    "\n",
    "#             # Print the results\n",
    "#             print(f\"Original graph: {result['original_graph']['different_label_edges']} edges with different labels \"\n",
    "#                   f\"out of {result['original_graph']['total_edges']} total edges \"\n",
    "#                   f\"({100-result['original_graph']['percentage']:.2f}%).\")\n",
    "\n",
    "#             print(f\"Sampled graph: {result['sampled_graph']['different_label_edges']} edges with different labels \"\n",
    "#                   f\"out of {result['sampled_graph']['total_edges']} total edges \"\n",
    "#                   f\"({100-result['sampled_graph']['percentage']:.2f}%).\")\n",
    "            \n",
    "    \n",
    "            # Print the results\n",
    "            print(f\"Original graph: {result['original_graph']['different_label_edges']} edges with different labels \"\n",
    "                  f\"out of {result['original_graph']['total_edges']} total edges \"\n",
    "                  f\"(Edge homophily: {100-result['original_graph']['percentage']:.2f}%).\")\n",
    "\n",
    "            print(f\"Sampled graph: {result['sampled_graph']['different_label_edges']} edges with different labels \"\n",
    "                  f\"out of {result['sampled_graph']['total_edges']} total edges \"\n",
    "                  f\"(Edge homophily: {100-result['sampled_graph']['percentage']:.2f}%).\")\n",
    "            \n",
    "    \n",
    "    \n",
    "            return fig\n",
    "            \n",
    "# fig = visualize(False) #Sampled from the best\n",
    "fig = visualize(True)  #Best Sparsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_gumbel(shape, eps=1e-20):\n",
    "#     \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "#     U = torch.rand(shape)\n",
    "#     return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "# # Example usage\n",
    "# shape = (10,)  # Sample 10 values\n",
    "# gumbel_noise = sample_gumbel(shape)\n",
    "# print(gumbel_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animate the Sparsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_figures_during_test():\n",
    "    figures = []\n",
    "    for epoch in range(20):  # Assuming 100 epochs\n",
    "        fig = visualize(False, False)\n",
    "        figures.append(fig)\n",
    "        plt.close(fig)\n",
    "    return figures\n",
    "\n",
    "# figures = store_figures_during_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def create_animation_from_figures(figures):\n",
    "    fig_anim, ax_anim = plt.subplots()\n",
    "\n",
    "    def update_fig(i):\n",
    "        ax_anim.clear()\n",
    "        figures[i].canvas.draw()\n",
    "        img_array = np.frombuffer(figures[i].canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(figures[i].canvas.get_width_height()[::-1] + (3,))\n",
    "        ax_anim.imshow(img_array)\n",
    "        ax_anim.axis('off')\n",
    "\n",
    "    ani = animation.FuncAnimation(fig_anim, update_fig, frames=len(figures), interval=200, blit=False)\n",
    "\n",
    "    from IPython.display import HTML\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "# # Example Usage:\n",
    "# # Assuming `figures` is a list of stored figure objects from `store_figures_during_training`.\n",
    "# animation_html = create_animation_from_figures(figures)\n",
    "# display(animation_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if figure_log:\n",
    "    animation_html = create_animation_from_figures(train_figures)\n",
    "    display(animation_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (My py311cu117pyg200 Kernel)",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
