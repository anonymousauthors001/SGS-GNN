{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2614eed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric.index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m index2ptr, ptr2index\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fs\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyg_lib\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric.index'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "\n",
    "import torch_geometric.typing\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.index import index2ptr, ptr2index\n",
    "from torch_geometric.io import fs\n",
    "from torch_geometric.typing import pyg_lib\n",
    "from torch_geometric.utils import index_sort, narrow, select, sort_edge_index\n",
    "from torch_geometric.utils.map import map_index\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Partition:\n",
    "    indptr: Tensor\n",
    "    index: Tensor\n",
    "    partptr: Tensor\n",
    "    node_perm: Tensor\n",
    "    edge_perm: Tensor\n",
    "    sparse_format: Literal['csr', 'csc']\n",
    "\n",
    "\n",
    "class ClusterData(torch.utils.data.Dataset):\n",
    "    r\"\"\"Clusters/partitions a graph data object into multiple subgraphs, as\n",
    "    motivated by the `\"Cluster-GCN: An Efficient Algorithm for Training Deep\n",
    "    and Large Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1905.07953>`_ paper.\n",
    "\n",
    "    .. note::\n",
    "        The underlying METIS algorithm requires undirected graphs as input.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): The graph data object.\n",
    "        num_parts (int): The number of partitions.\n",
    "        recursive (bool, optional): If set to :obj:`True`, will use multilevel\n",
    "            recursive bisection instead of multilevel k-way partitioning.\n",
    "            (default: :obj:`False`)\n",
    "        save_dir (str, optional): If set, will save the partitioned data to the\n",
    "            :obj:`save_dir` directory for faster re-use. (default: :obj:`None`)\n",
    "        filename (str, optional): Name of the stored partitioned file.\n",
    "            (default: :obj:`None`)\n",
    "        log (bool, optional): If set to :obj:`False`, will not log any\n",
    "            progress. (default: :obj:`True`)\n",
    "        keep_inter_cluster_edges (bool, optional): If set to :obj:`True`,\n",
    "            will keep inter-cluster edge connections. (default: :obj:`False`)\n",
    "        sparse_format (str, optional): The sparse format to use for computing\n",
    "            partitions. (default: :obj:`\"csr\"`)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        num_parts: int,\n",
    "        recursive: bool = False,\n",
    "        save_dir: Optional[str] = None,\n",
    "        filename: Optional[str] = None,\n",
    "        log: bool = True,\n",
    "        keep_inter_cluster_edges: bool = False,\n",
    "        sparse_format: Literal['csr', 'csc'] = 'csr',\n",
    "    ):\n",
    "        assert data.edge_index is not None\n",
    "        assert sparse_format in ['csr', 'csc']\n",
    "\n",
    "        self.num_parts = num_parts\n",
    "        self.recursive = recursive\n",
    "        self.keep_inter_cluster_edges = keep_inter_cluster_edges\n",
    "        self.sparse_format = sparse_format\n",
    "\n",
    "        recursive_str = '_recursive' if recursive else ''\n",
    "        root_dir = osp.join(save_dir or '', f'part_{num_parts}{recursive_str}')\n",
    "        path = osp.join(root_dir, filename or 'metis.pt')\n",
    "\n",
    "        if save_dir is not None and osp.exists(path):\n",
    "            self.partition = fs.torch_load(path)\n",
    "        else:\n",
    "            if log:  # pragma: no cover\n",
    "                print('Computing METIS partitioning...', file=sys.stderr)\n",
    "\n",
    "            cluster = self._metis(data.edge_index, data.num_nodes)\n",
    "            self.partition = self._partition(data.edge_index, cluster)\n",
    "\n",
    "            if save_dir is not None:\n",
    "                os.makedirs(root_dir, exist_ok=True)\n",
    "                torch.save(self.partition, path)\n",
    "\n",
    "            if log:  # pragma: no cover\n",
    "                print('Done!', file=sys.stderr)\n",
    "\n",
    "        self.data = self._permute_data(data, self.partition)\n",
    "\n",
    "    def _metis(self, edge_index: Tensor, num_nodes: int) -> Tensor:\n",
    "        # Computes a node-level partition assignment vector via METIS.\n",
    "        if self.sparse_format == 'csr':  # Calculate CSR representation:\n",
    "            row, index = sort_edge_index(edge_index, num_nodes=num_nodes)\n",
    "            indptr = index2ptr(row, size=num_nodes)\n",
    "        else:  # Calculate CSC representation:\n",
    "            index, col = sort_edge_index(edge_index, num_nodes=num_nodes,\n",
    "                                         sort_by_row=False)\n",
    "            indptr = index2ptr(col, size=num_nodes)\n",
    "\n",
    "        # Compute METIS partitioning:\n",
    "        cluster: Optional[Tensor] = None\n",
    "\n",
    "        if torch_geometric.typing.WITH_TORCH_SPARSE:\n",
    "            try:\n",
    "                cluster = torch.ops.torch_sparse.partition(\n",
    "                    indptr.cpu(),\n",
    "                    index.cpu(),\n",
    "                    None,\n",
    "                    self.num_parts,\n",
    "                    self.recursive,\n",
    "                ).to(edge_index.device)\n",
    "            except (AttributeError, RuntimeError):\n",
    "                pass\n",
    "\n",
    "        if cluster is None and torch_geometric.typing.WITH_METIS:\n",
    "            cluster = pyg_lib.partition.metis(\n",
    "                indptr.cpu(),\n",
    "                index.cpu(),\n",
    "                self.num_parts,\n",
    "                recursive=self.recursive,\n",
    "            ).to(edge_index.device)\n",
    "\n",
    "        if cluster is None:\n",
    "            raise ImportError(f\"'{self.__class__.__name__}' requires either \"\n",
    "                              f\"'pyg-lib' or 'torch-sparse'\")\n",
    "\n",
    "        return cluster\n",
    "\n",
    "    def _partition(self, edge_index: Tensor, cluster: Tensor) -> Partition:\n",
    "        # Computes node-level and edge-level permutations and permutes the edge\n",
    "        # connectivity accordingly:\n",
    "\n",
    "        # Sort `cluster` and compute boundaries `partptr`:\n",
    "        cluster, node_perm = index_sort(cluster, max_value=self.num_parts)\n",
    "        partptr = index2ptr(cluster, size=self.num_parts)\n",
    "\n",
    "        # Permute `edge_index` based on node permutation:\n",
    "        edge_perm = torch.arange(edge_index.size(1), device=edge_index.device)\n",
    "        arange = torch.empty_like(node_perm)\n",
    "        arange[node_perm] = torch.arange(cluster.numel(),\n",
    "                                         device=cluster.device)\n",
    "        edge_index = arange[edge_index]\n",
    "\n",
    "        # Compute final CSR representation:\n",
    "        (row, col), edge_perm = sort_edge_index(\n",
    "            edge_index,\n",
    "            edge_attr=edge_perm,\n",
    "            num_nodes=cluster.numel(),\n",
    "            sort_by_row=self.sparse_format == 'csr',\n",
    "        )\n",
    "        if self.sparse_format == 'csr':\n",
    "            indptr, index = index2ptr(row, size=cluster.numel()), col\n",
    "        else:\n",
    "            indptr, index = index2ptr(col, size=cluster.numel()), row\n",
    "\n",
    "        return Partition(indptr, index, partptr, node_perm, edge_perm,\n",
    "                         self.sparse_format)\n",
    "\n",
    "    def _permute_data(self, data: Data, partition: Partition) -> Data:\n",
    "        # Permute node-level and edge-level attributes according to the\n",
    "        # calculated permutations in `Partition`:\n",
    "        out = copy.copy(data)\n",
    "        for key, value in data.items():\n",
    "            if key == 'edge_index':\n",
    "                continue\n",
    "            elif data.is_node_attr(key):\n",
    "                cat_dim = data.__cat_dim__(key, value)\n",
    "                out[key] = select(value, partition.node_perm, dim=cat_dim)\n",
    "            elif data.is_edge_attr(key):\n",
    "                cat_dim = data.__cat_dim__(key, value)\n",
    "                out[key] = select(value, partition.edge_perm, dim=cat_dim)\n",
    "        out.edge_index = None\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.partition.partptr.numel() - 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Data:\n",
    "        node_start = int(self.partition.partptr[idx])\n",
    "        node_end = int(self.partition.partptr[idx + 1])\n",
    "        node_length = node_end - node_start\n",
    "\n",
    "        indptr = self.partition.indptr[node_start:node_end + 1]\n",
    "        edge_start = int(indptr[0])\n",
    "        edge_end = int(indptr[-1])\n",
    "        edge_length = edge_end - edge_start\n",
    "        indptr = indptr - edge_start\n",
    "\n",
    "        if self.sparse_format == 'csr':\n",
    "            row = ptr2index(indptr)\n",
    "            col = self.partition.index[edge_start:edge_end]\n",
    "            if not self.keep_inter_cluster_edges:\n",
    "                edge_mask = (col >= node_start) & (col < node_end)\n",
    "                row = row[edge_mask]\n",
    "                col = col[edge_mask] - node_start\n",
    "        else:\n",
    "            col = ptr2index(indptr)\n",
    "            row = self.partition.index[edge_start:edge_end]\n",
    "            if not self.keep_inter_cluster_edges:\n",
    "                edge_mask = (row >= node_start) & (row < node_end)\n",
    "                col = col[edge_mask]\n",
    "                row = row[edge_mask] - node_start\n",
    "\n",
    "        out = copy.copy(self.data)\n",
    "\n",
    "        for key, value in self.data.items():\n",
    "            if key == 'num_nodes':\n",
    "                out.num_nodes = node_length\n",
    "            elif self.data.is_node_attr(key):\n",
    "                cat_dim = self.data.__cat_dim__(key, value)\n",
    "                out[key] = narrow(value, cat_dim, node_start, node_length)\n",
    "            elif self.data.is_edge_attr(key):\n",
    "                cat_dim = self.data.__cat_dim__(key, value)\n",
    "                out[key] = narrow(value, cat_dim, edge_start, edge_length)\n",
    "                if not self.keep_inter_cluster_edges:\n",
    "                    out[key] = out[key][edge_mask]\n",
    "\n",
    "        out.edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}({self.num_parts})'\n",
    "\n",
    "\n",
    "class ClusterLoader(torch.utils.data.DataLoader):\n",
    "    r\"\"\"The data loader scheme from the `\"Cluster-GCN: An Efficient Algorithm\n",
    "    for Training Deep and Large Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1905.07953>`_ paper which merges partioned subgraphs\n",
    "    and their between-cluster links from a large-scale graph data object to\n",
    "    form a mini-batch.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        Use :class:`~torch_geometric.loader.ClusterData` and\n",
    "        :class:`~torch_geometric.loader.ClusterLoader` in conjunction to\n",
    "        form mini-batches of clusters.\n",
    "        For an example of using Cluster-GCN, see\n",
    "        `examples/cluster_gcn_reddit.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py>`_ or\n",
    "        `examples/cluster_gcn_ppi.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/cluster_gcn_ppi.py>`_.\n",
    "\n",
    "    Args:\n",
    "        cluster_data (torch_geometric.loader.ClusterData): The already\n",
    "            partioned data object.\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch.utils.data.DataLoader`, such as :obj:`batch_size`,\n",
    "            :obj:`shuffle`, :obj:`drop_last` or :obj:`num_workers`.\n",
    "    \"\"\"\n",
    "    def __init__(self, cluster_data, **kwargs):\n",
    "        self.cluster_data = cluster_data\n",
    "        iterator = range(len(cluster_data))\n",
    "        super().__init__(iterator, collate_fn=self._collate, **kwargs)\n",
    "\n",
    "    def _collate(self, batch: List[int]) -> Data:\n",
    "        if not isinstance(batch, torch.Tensor):\n",
    "            batch = torch.tensor(batch)\n",
    "\n",
    "        global_indptr = self.cluster_data.partition.indptr\n",
    "        global_index = self.cluster_data.partition.index\n",
    "\n",
    "        # Get all node-level and edge-level start and end indices for the\n",
    "        # current mini-batch:\n",
    "        node_start = self.cluster_data.partition.partptr[batch]\n",
    "        node_end = self.cluster_data.partition.partptr[batch + 1]\n",
    "        edge_start = global_indptr[node_start]\n",
    "        edge_end = global_indptr[node_end]\n",
    "\n",
    "        # Iterate over each partition in the batch and calculate new edge\n",
    "        # connectivity. This is done by slicing the corresponding source and\n",
    "        # destination indices for each partition and adjusting their indices to\n",
    "        # start from zero:\n",
    "        rows, cols, nodes, cumsum = [], [], [], 0\n",
    "        for i in range(batch.numel()):\n",
    "            nodes.append(torch.arange(node_start[i], node_end[i]))\n",
    "            indptr = global_indptr[node_start[i]:node_end[i] + 1]\n",
    "            indptr = indptr - edge_start[i]\n",
    "            if self.cluster_data.partition.sparse_format == 'csr':\n",
    "                row = ptr2index(indptr) + cumsum\n",
    "                col = global_index[edge_start[i]:edge_end[i]]\n",
    "\n",
    "            else:\n",
    "                col = ptr2index(indptr) + cumsum\n",
    "                row = global_index[edge_start[i]:edge_end[i]]\n",
    "\n",
    "            rows.append(row)\n",
    "            cols.append(col)\n",
    "            cumsum += indptr.numel() - 1\n",
    "\n",
    "        node = torch.cat(nodes, dim=0)\n",
    "        row = torch.cat(rows, dim=0)\n",
    "        col = torch.cat(cols, dim=0)\n",
    "\n",
    "        # Map `col` vector to valid entries and remove any entries that do not\n",
    "        # connect two nodes within the same mini-batch:\n",
    "        if self.cluster_data.partition.sparse_format == 'csr':\n",
    "            col, edge_mask = map_index(col, node)\n",
    "            row = row[edge_mask]\n",
    "        else:\n",
    "            row, edge_mask = map_index(row, node)\n",
    "            col = col[edge_mask]\n",
    "        out = copy.copy(self.cluster_data.data)\n",
    "\n",
    "        # Slice node-level and edge-level attributes according to its offsets:\n",
    "        for key, value in self.cluster_data.data.items():\n",
    "            if key == 'num_nodes':\n",
    "                out.num_nodes = cumsum\n",
    "            elif self.cluster_data.data.is_node_attr(key):\n",
    "                cat_dim = self.cluster_data.data.__cat_dim__(key, value)\n",
    "                out[key] = torch.cat([\n",
    "                    narrow(out[key], cat_dim, s, e - s)\n",
    "                    for s, e in zip(node_start, node_end)\n",
    "                ], dim=cat_dim)\n",
    "            elif self.cluster_data.data.is_edge_attr(key):\n",
    "                cat_dim = self.cluster_data.data.__cat_dim__(key, value)\n",
    "                value = torch.cat([\n",
    "                    narrow(out[key], cat_dim, s, e - s)\n",
    "                    for s, e in zip(edge_start, edge_end)\n",
    "                ], dim=cat_dim)\n",
    "                out[key] = select(value, edge_mask, dim=cat_dim)\n",
    "\n",
    "        out.edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53dd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "454a3719",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m Data(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, edge_index \u001b[38;5;241m=\u001b[39m edge_index)\n\u001b[0;32m---> 12\u001b[0m cluster_dir \u001b[38;5;241m=\u001b[39m DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/dummy/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# os.mkdir(cluster_dir)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m cluster_data \u001b[38;5;241m=\u001b[39m ClusterData(data, num_parts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save_dir\u001b[38;5;241m=\u001b[39mcluster_dir, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DIR' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "    y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "    edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "    # edge_index = torch.LongTensor([[1,2]]).T\n",
    "\n",
    "    edge_index = edge_index-1\n",
    "    data = Data(x=x, y=y, edge_index = edge_index)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cluster_dir = '/scratch/gilbreth/das90/Dataset/tmp/dummy/'\n",
    "    # os.mkdir(cluster_dir)\n",
    "    cluster_data = ClusterData(data, num_parts=2, recursive=False, save_dir=cluster_dir, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eaa3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py311cu117pyg200 Kernel)",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
