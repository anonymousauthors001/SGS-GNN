{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220c565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce76277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def one_hot(\n",
    "    index: Tensor,\n",
    "    num_classes: Optional[int] = None,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Taskes a one-dimensional :obj:`index` tensor and returns a one-hot\n",
    "    encoded representation of it with shape :obj:`[*, num_classes]` that has\n",
    "    zeros everywhere except where the index of last dimension matches the\n",
    "    corresponding value of the input tensor, in which case it will be :obj:`1`.\n",
    "\n",
    "    .. note::\n",
    "        This is a more memory-efficient version of\n",
    "        :meth:`torch.nn.functional.one_hot` as you can customize the output\n",
    "        :obj:`dtype`.\n",
    "\n",
    "    Args:\n",
    "        index (torch.Tensor): The one-dimensional input tensor.\n",
    "        num_classes (int, optional): The total number of classes. If set to\n",
    "            :obj:`None`, the number of classes will be inferred as one greater\n",
    "            than the largest class value in the input tensor.\n",
    "            (default: :obj:`None`)\n",
    "        dtype (torch.dtype, optional): The :obj:`dtype` of the output tensor.\n",
    "    \"\"\"\n",
    "    if index.dim() != 1:\n",
    "        raise ValueError(\"'index' tensor needs to be one-dimensional\")\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = int(index.max()) + 1\n",
    "\n",
    "    out = torch.zeros((index.size(0), num_classes), dtype=dtype,\n",
    "                      device=index.device)\n",
    "    return out.scatter_(1, index.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e2295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import ssl\n",
    "import sys\n",
    "import urllib\n",
    "from typing import Optional\n",
    "\n",
    "from torch_geometric.data.makedirs import makedirs\n",
    "\n",
    "\n",
    "def download_url(url: str, folder: str, log: bool = True,\n",
    "                 filename: Optional[str] = None):\n",
    "    r\"\"\"Downloads the content of an URL to a specific folder.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL.\n",
    "        folder (str): The folder.\n",
    "        log (bool, optional): If :obj:`False`, will not print anything to the\n",
    "            console. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    if filename is None:\n",
    "        filename = url.rpartition('/')[2]\n",
    "        filename = filename if filename[0] == '?' else filename.split('?')[0]\n",
    "\n",
    "    path = osp.join(folder, filename)\n",
    "\n",
    "    if osp.exists(path):  # pragma: no cover\n",
    "        if log and 'pytest' not in sys.modules:\n",
    "            print(f'Using existing file {filename}', file=sys.stderr)\n",
    "        return path\n",
    "\n",
    "    if log and 'pytest' not in sys.modules:\n",
    "        print(f'Downloading {url}', file=sys.stderr)\n",
    "\n",
    "    makedirs(folder)\n",
    "\n",
    "    context = ssl._create_unverified_context()\n",
    "    data = urllib.request.urlopen(url, context=context)\n",
    "\n",
    "    with open(path, 'wb') as f:\n",
    "        # workaround for https://bugs.python.org/issue42853\n",
    "        while True:\n",
    "            chunk = data.read(10 * 1024 * 1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7b200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset #download_url\n",
    "#from torch_geometric.utils import one_hot\n",
    "\n",
    "\n",
    "class LINKXDataset(InMemoryDataset):\n",
    "    r\"\"\"A variety of non-homophilous graph datasets from the `\"Large Scale\n",
    "    Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple\n",
    "    Methods\" <https://arxiv.org/abs/2110.14446>`_ paper.\n",
    "\n",
    "    .. note::\n",
    "        Some of the datasets provided in :class:`LINKXDataset` are from other\n",
    "        sources, but have been updated with new features and/or labels.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        name (str): The name of the dataset (:obj:`\"penn94\"`, :obj:`\"reed98\"`,\n",
    "            :obj:`\"amherst41\"`, :obj:`\"cornell5\"`, :obj:`\"johnshopkins55\"`,\n",
    "            :obj:`\"genius\"`).\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    github_url = ('https://github.com/CUAI/Non-Homophily-Large-Scale/'\n",
    "                  'raw/master/data')\n",
    "    gdrive_url = 'https://drive.google.com/uc?confirm=t&'\n",
    "\n",
    "    facebook_datasets = [\n",
    "        'penn94', 'reed98', 'amherst41', 'cornell5', 'johnshopkins55'\n",
    "    ]\n",
    "\n",
    "    datasets = {\n",
    "        'penn94': {\n",
    "            'data.mat': f'{github_url}/facebook100/Penn94.mat'\n",
    "        },\n",
    "        'reed98': {\n",
    "            'data.mat': f'{github_url}/facebook100/Reed98.mat'\n",
    "        },\n",
    "        'amherst41': {\n",
    "            'data.mat': f'{github_url}/facebook100/Amherst41.mat',\n",
    "        },\n",
    "        'cornell5': {\n",
    "            'data.mat': f'{github_url}/facebook100/Cornell5.mat'\n",
    "        },\n",
    "        'johnshopkins55': {\n",
    "            'data.mat': f'{github_url}/facebook100/Johns%20Hopkins55.mat'\n",
    "        },\n",
    "        'genius': {\n",
    "            'data.mat': f'{github_url}/genius.mat'\n",
    "        },\n",
    "        'wiki': {\n",
    "            'wiki_views2M.pt':\n",
    "            f'{gdrive_url}id=1p5DlVHrnFgYm3VsNIzahSsvCD424AyvP',\n",
    "            'wiki_edges2M.pt':\n",
    "            f'{gdrive_url}id=14X7FlkjrlUgmnsYtPwdh-gGuFla4yb5u',\n",
    "            'wiki_features2M.pt':\n",
    "            f'{gdrive_url}id=1ySNspxbK-snNoAZM7oxiWGvOnTRdSyEK'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    splits = {\n",
    "        'penn94': f'{github_url}/splits/fb100-Penn94-splits.npy',\n",
    "    }\n",
    "\n",
    "    def __init__(self, root: str, name: str,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        self.name = name.lower()\n",
    "        assert self.name in self.datasets.keys()\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        names = list(self.datasets[self.name].keys())\n",
    "        if self.name in self.splits:\n",
    "            names += [self.splits[self.name].split('/')[-1]]\n",
    "        return names\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        for filename, path in self.datasets[self.name].items():\n",
    "            download_url(path, self.raw_dir, filename=filename)\n",
    "        if self.name in self.splits:\n",
    "            download_url(self.splits[self.name], self.raw_dir)\n",
    "\n",
    "    def _process_wiki(self):\n",
    "\n",
    "        paths = {x.split('/')[-1]: x for x in self.raw_paths}\n",
    "        x = torch.load(paths['wiki_features2M.pt'])\n",
    "        edge_index = torch.load(paths['wiki_edges2M.pt']).t().contiguous()\n",
    "        y = torch.load(paths['wiki_views2M.pt'])\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "    def _process_facebook(self):\n",
    "        from scipy.io import loadmat\n",
    "\n",
    "        mat = loadmat(self.raw_paths[0])\n",
    "\n",
    "        A = mat['A'].tocsr().tocoo()\n",
    "        row = torch.from_numpy(A.row).to(torch.long)\n",
    "        col = torch.from_numpy(A.col).to(torch.long)\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        metadata = torch.from_numpy(mat['local_info'].astype('int64'))\n",
    "\n",
    "        xs = []\n",
    "        y = metadata[:, 1] - 1  # gender label, -1 means unlabeled\n",
    "        x = torch.cat([metadata[:, :1], metadata[:, 2:]], dim=-1)\n",
    "        for i in range(x.size(1)):\n",
    "            _, out = x[:, i].unique(return_inverse=True)\n",
    "            xs.append(one_hot(out))\n",
    "        x = torch.cat(xs, dim=-1)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "        if self.name in self.splits:\n",
    "            splits = np.load(self.raw_paths[1], allow_pickle=True)\n",
    "            sizes = (data.num_nodes, len(splits))\n",
    "            data.train_mask = torch.zeros(sizes, dtype=torch.bool)\n",
    "            data.val_mask = torch.zeros(sizes, dtype=torch.bool)\n",
    "            data.test_mask = torch.zeros(sizes, dtype=torch.bool)\n",
    "\n",
    "            for i, split in enumerate(splits):\n",
    "                data.train_mask[:, i][torch.tensor(split['train'])] = True\n",
    "                data.val_mask[:, i][torch.tensor(split['valid'])] = True\n",
    "                data.test_mask[:, i][torch.tensor(split['test'])] = True\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _process_genius(self):\n",
    "        from scipy.io import loadmat\n",
    "\n",
    "        mat = loadmat(self.raw_paths[0])\n",
    "        edge_index = torch.from_numpy(mat['edge_index']).to(torch.long)\n",
    "        x = torch.from_numpy(mat['node_feat']).to(torch.float)\n",
    "        y = torch.from_numpy(mat['label']).squeeze().to(torch.long)\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "    def process(self):\n",
    "        if self.name in self.facebook_datasets:\n",
    "            data = self._process_facebook()\n",
    "        elif self.name == 'genius':\n",
    "            data = self._process_genius()\n",
    "        elif self.name == 'wiki':\n",
    "            data = self._process_wiki()\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"chosen dataset '{self.name}' is not implemented\")\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.name.capitalize()}({len(self)})'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8dc746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "#, download_url\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "\n",
    "class HeterophilousGraphDataset(InMemoryDataset):\n",
    "    r\"\"\"The heterophilous graphs :obj:`\"Roman-empire\"`,\n",
    "    :obj:`\"Amazon-ratings\"`, :obj:`\"Minesweeper\"`, :obj:`\"Tolokers\"` and\n",
    "    :obj:`\"Questions\"` from the `\"A Critical Look at the Evaluation of GNNs\n",
    "    under Heterophily: Are We Really Making Progress?\"\n",
    "    <https://arxiv.org/abs/2302.11640>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        name (str): The name of the dataset (:obj:`\"Roman-empire\"`,\n",
    "            :obj:`\"Amazon-ratings\"`, :obj:`\"Minesweeper\"`, :obj:`\"Tolokers\"`,\n",
    "            :obj:`\"Questions\"`).\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "\n",
    "    **STATS:**\n",
    "\n",
    "    .. list-table::\n",
    "        :widths: 10 10 10 10 10\n",
    "        :header-rows: 1\n",
    "\n",
    "        * - Name\n",
    "          - #nodes\n",
    "          - #edges\n",
    "          - #features\n",
    "          - #classes\n",
    "        * - Roman-empire\n",
    "          - 22,662\n",
    "          - 32,927\n",
    "          - 300\n",
    "          - 18\n",
    "        * - Amazon-ratings\n",
    "          - 24,492\n",
    "          - 93,050\n",
    "          - 300\n",
    "          - 5\n",
    "        * - Minesweeper\n",
    "          - 10,000\n",
    "          - 39,402\n",
    "          - 7\n",
    "          - 2\n",
    "        * - Tolokers\n",
    "          - 11,758\n",
    "          - 519,000\n",
    "          - 10\n",
    "          - 2\n",
    "        * - Questions\n",
    "          - 48,921\n",
    "          - 153,540\n",
    "          - 301\n",
    "          - 2\n",
    "    \"\"\"\n",
    "    url = ('https://github.com/yandex-research/heterophilous-graphs/raw/'\n",
    "           'main/data')\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        name: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.name = name.lower().replace('-', '_')\n",
    "        assert self.name in [\n",
    "            'roman_empire',\n",
    "            'amazon_ratings',\n",
    "            'minesweeper',\n",
    "            'tolokers',\n",
    "            'questions',\n",
    "        ]\n",
    "\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> str:\n",
    "        return f'{self.name}.npz'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        download_url(f'{self.url}/{self.name}.npz', self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        raw = np.load(self.raw_paths[0], 'r')\n",
    "        x = torch.from_numpy(raw['node_features'])\n",
    "        y = torch.from_numpy(raw['node_labels'])\n",
    "        edge_index = torch.from_numpy(raw['edges']).t().contiguous()\n",
    "        edge_index = to_undirected(edge_index, num_nodes=x.size(0))\n",
    "        train_mask = torch.from_numpy(raw['train_masks']).t().contiguous()\n",
    "        val_mask = torch.from_numpy(raw['val_masks']).t().contiguous()\n",
    "        test_mask = torch.from_numpy(raw['test_masks']).t().contiguous()\n",
    "\n",
    "        data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask,\n",
    "                    val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(name={self.name})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42724f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.utils import coalesce\n",
    "\n",
    "\n",
    "class Actor(InMemoryDataset):\n",
    "    r\"\"\"The actor-only induced subgraph of the film-director-actor-writer\n",
    "    network used in the\n",
    "    `\"Geom-GCN: Geometric Graph Convolutional Networks\"\n",
    "    <https://openreview.net/forum?id=S1e2agrFvS>`_ paper.\n",
    "    Each node corresponds to an actor, and the edge between two nodes denotes\n",
    "    co-occurrence on the same Wikipedia page.\n",
    "    Node features correspond to some keywords in the Wikipedia pages.\n",
    "    The task is to classify the nodes into five categories in term of words of\n",
    "    actor's Wikipedia.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "\n",
    "    **STATS:**\n",
    "\n",
    "    .. list-table::\n",
    "        :widths: 10 10 10 10\n",
    "        :header-rows: 1\n",
    "\n",
    "        * - #nodes\n",
    "          - #edges\n",
    "          - #features\n",
    "          - #classes\n",
    "        * - 7,600\n",
    "          - 30,019\n",
    "          - 932\n",
    "          - 5\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master'\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return ['out1_node_feature_label.txt', 'out1_graph_edges.txt'\n",
    "                ] + [f'film_split_0.6_0.2_{i}.npz' for i in range(10)]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        for f in self.raw_file_names[:2]:\n",
    "            download_url(f'{self.url}/new_data/film/{f}', self.raw_dir)\n",
    "        for f in self.raw_file_names[2:]:\n",
    "            download_url(f'{self.url}/splits/{f}', self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        with open(self.raw_paths[0], 'r') as f:\n",
    "            data = [x.split('\\t') for x in f.read().split('\\n')[1:-1]]\n",
    "\n",
    "            rows, cols = [], []\n",
    "            for n_id, col, _ in data:\n",
    "                col = [int(x) for x in col.split(',')]\n",
    "                rows += [int(n_id)] * len(col)\n",
    "                cols += col\n",
    "            row, col = torch.tensor(rows), torch.tensor(cols)\n",
    "\n",
    "            x = torch.zeros(int(row.max()) + 1, int(col.max()) + 1)\n",
    "            x[row, col] = 1.\n",
    "\n",
    "            y = torch.empty(len(data), dtype=torch.long)\n",
    "            for n_id, _, label in data:\n",
    "                y[int(n_id)] = int(label)\n",
    "\n",
    "        with open(self.raw_paths[1], 'r') as f:\n",
    "            data = f.read().split('\\n')[1:-1]\n",
    "            data = [[int(v) for v in r.split('\\t')] for r in data]\n",
    "            edge_index = torch.tensor(data, dtype=torch.long).t().contiguous()\n",
    "            edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        train_masks, val_masks, test_masks = [], [], []\n",
    "        for f in self.raw_paths[2:]:\n",
    "            tmp = np.load(f)\n",
    "            train_masks += [torch.from_numpy(tmp['train_mask']).to(torch.bool)]\n",
    "            val_masks += [torch.from_numpy(tmp['val_mask']).to(torch.bool)]\n",
    "            test_masks += [torch.from_numpy(tmp['test_mask']).to(torch.bool)]\n",
    "        train_mask = torch.stack(train_masks, dim=1)\n",
    "        val_mask = torch.stack(val_masks, dim=1)\n",
    "        test_mask = torch.stack(test_masks, dim=1)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask,\n",
    "                    val_mask=val_mask, test_mask=test_mask)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "850982fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebKB(InMemoryDataset):\n",
    "    r\"\"\"The WebKB datasets used in the\n",
    "    `\"Geom-GCN: Geometric Graph Convolutional Networks\"\n",
    "    <https://openreview.net/forum?id=S1e2agrFvS>`_ paper.\n",
    "    Nodes represent web pages and edges represent hyperlinks between them.\n",
    "    Node features are the bag-of-words representation of web pages.\n",
    "    The task is to classify the nodes into one of the five categories, student,\n",
    "    project, course, staff, and faculty.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        name (str): The name of the dataset (:obj:`\"Cornell\"`, :obj:`\"Texas\"`,\n",
    "            :obj:`\"Wisconsin\"`).\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "\n",
    "    **STATS:**\n",
    "\n",
    "    .. list-table::\n",
    "        :widths: 10 10 10 10 10\n",
    "        :header-rows: 1\n",
    "\n",
    "        * - Name\n",
    "          - #nodes\n",
    "          - #edges\n",
    "          - #features\n",
    "          - #classes\n",
    "        * - Cornell\n",
    "          - 183\n",
    "          - 298\n",
    "          - 1,703\n",
    "          - 5\n",
    "        * - Texas\n",
    "          - 183\n",
    "          - 325\n",
    "          - 1,703\n",
    "          - 5\n",
    "        * - Wisconsin\n",
    "          - 251\n",
    "          - 515\n",
    "          - 1,703\n",
    "          - 5\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        name: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.name = name.lower()\n",
    "        assert self.name in ['cornell', 'texas', 'wisconsin']\n",
    "\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        out = ['out1_node_feature_label.txt', 'out1_graph_edges.txt']\n",
    "        out += [f'{self.name}_split_0.6_0.2_{i}.npz' for i in range(10)]\n",
    "        return out\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        for f in self.raw_file_names[:2]:\n",
    "            download_url(f'{self.url}/new_data/{self.name}/{f}', self.raw_dir)\n",
    "        for f in self.raw_file_names[2:]:\n",
    "            download_url(f'{self.url}/splits/{f}', self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        with open(self.raw_paths[0], 'r') as f:\n",
    "            data = f.read().split('\\n')[1:-1]\n",
    "            x = [[float(v) for v in r.split('\\t')[1].split(',')] for r in data]\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "            y = [int(r.split('\\t')[2]) for r in data]\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        with open(self.raw_paths[1], 'r') as f:\n",
    "            data = f.read().split('\\n')[1:-1]\n",
    "            data = [[int(v) for v in r.split('\\t')] for r in data]\n",
    "            edge_index = torch.tensor(data, dtype=torch.long).t().contiguous()\n",
    "            edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        train_masks, val_masks, test_masks = [], [], []\n",
    "        for f in self.raw_paths[2:]:\n",
    "            tmp = np.load(f)\n",
    "            train_masks += [torch.from_numpy(tmp['train_mask']).to(torch.bool)]\n",
    "            val_masks += [torch.from_numpy(tmp['val_mask']).to(torch.bool)]\n",
    "            test_masks += [torch.from_numpy(tmp['test_mask']).to(torch.bool)]\n",
    "        train_mask = torch.stack(train_masks, dim=1)\n",
    "        val_mask = torch.stack(val_masks, dim=1)\n",
    "        test_mask = torch.stack(test_masks, dim=1)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask,\n",
    "                    val_mask=val_mask, test_mask=test_mask)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.name}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a25ef160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaNetwork(InMemoryDataset):\n",
    "    r\"\"\"The Wikipedia networks introduced in the\n",
    "    `\"Multi-scale Attributed Node Embedding\"\n",
    "    <https://arxiv.org/abs/1909.13021>`_ paper.\n",
    "    Nodes represent web pages and edges represent hyperlinks between them.\n",
    "    Node features represent several informative nouns in the Wikipedia pages.\n",
    "    The task is to predict the average daily traffic of the web page.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        name (str): The name of the dataset (:obj:`\"chameleon\"`,\n",
    "            :obj:`\"crocodile\"`, :obj:`\"squirrel\"`).\n",
    "        geom_gcn_preprocess (bool): If set to :obj:`True`, will load the\n",
    "            pre-processed data as introduced in the `\"Geom-GCN: Geometric\n",
    "            Graph Convolutional Networks\" <https://arxiv.org/abs/2002.05287>_`,\n",
    "            in which the average monthly traffic of the web page is converted\n",
    "            into five categories to predict.\n",
    "            If set to :obj:`True`, the dataset :obj:`\"crocodile\"` is not\n",
    "            available.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    raw_url = 'https://graphmining.ai/datasets/ptg/wiki'\n",
    "    processed_url = ('https://raw.githubusercontent.com/graphdml-uiuc-jlu/'\n",
    "                     'geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f')\n",
    "\n",
    "    def __init__(self, root: str, name: str, geom_gcn_preprocess: bool = True,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        self.name = name.lower()\n",
    "        self.geom_gcn_preprocess = geom_gcn_preprocess\n",
    "        assert self.name in ['chameleon', 'crocodile', 'squirrel']\n",
    "        if geom_gcn_preprocess and self.name == 'crocodile':\n",
    "            raise AttributeError(\"The dataset 'crocodile' is not available in \"\n",
    "                                 \"case 'geom_gcn_preprocess=True'\")\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        if self.geom_gcn_preprocess:\n",
    "            return osp.join(self.root, self.name, 'geom_gcn', 'raw')\n",
    "        else:\n",
    "            return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        if self.geom_gcn_preprocess:\n",
    "            return osp.join(self.root, self.name, 'geom_gcn', 'processed')\n",
    "        else:\n",
    "            return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> str:\n",
    "        if self.geom_gcn_preprocess:\n",
    "            return (['out1_node_feature_label.txt', 'out1_graph_edges.txt'] +\n",
    "                    [f'{self.name}_split_0.6_0.2_{i}.npz' for i in range(10)])\n",
    "        else:\n",
    "            return f'{self.name}.npz'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        if self.geom_gcn_preprocess:\n",
    "            for filename in self.raw_file_names[:2]:\n",
    "                url = f'{self.processed_url}/new_data/{self.name}/{filename}'\n",
    "                download_url(url, self.raw_dir)\n",
    "            for filename in self.raw_file_names[2:]:\n",
    "                url = f'{self.processed_url}/splits/{filename}'\n",
    "                download_url(url, self.raw_dir)\n",
    "        else:\n",
    "            download_url(f'{self.raw_url}/{self.name}.npz', self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        if self.geom_gcn_preprocess:\n",
    "            with open(self.raw_paths[0], 'r') as f:\n",
    "                data = f.read().split('\\n')[1:-1]\n",
    "            x = [[float(v) for v in r.split('\\t')[1].split(',')] for r in data]\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "            y = [int(r.split('\\t')[2]) for r in data]\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "            with open(self.raw_paths[1], 'r') as f:\n",
    "                data = f.read().split('\\n')[1:-1]\n",
    "                data = [[int(v) for v in r.split('\\t')] for r in data]\n",
    "            edge_index = torch.tensor(data, dtype=torch.long).t().contiguous()\n",
    "            edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "            train_masks, val_masks, test_masks = [], [], []\n",
    "            for filepath in self.raw_paths[2:]:\n",
    "                f = np.load(filepath)\n",
    "                train_masks += [torch.from_numpy(f['train_mask'])]\n",
    "                val_masks += [torch.from_numpy(f['val_mask'])]\n",
    "                test_masks += [torch.from_numpy(f['test_mask'])]\n",
    "            train_mask = torch.stack(train_masks, dim=1).to(torch.bool)\n",
    "            val_mask = torch.stack(val_masks, dim=1).to(torch.bool)\n",
    "            test_mask = torch.stack(test_masks, dim=1).to(torch.bool)\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask,\n",
    "                        val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "        else:\n",
    "            data = np.load(self.raw_paths[0], 'r', allow_pickle=True)\n",
    "            x = torch.from_numpy(data['features']).to(torch.float)\n",
    "            edge_index = torch.from_numpy(data['edges']).to(torch.long)\n",
    "            edge_index = edge_index.t().contiguous()\n",
    "            edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "            y = torch.from_numpy(data['target']).to(torch.float)\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f8093e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/cornell/out1_node_feature_label.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/cornell/out1_graph_edges.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_0.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_1.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_2.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_3.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_4.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_5.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_6.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_7.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_8.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_9.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_node_feature_label.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cornell()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_graph_edges.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_0.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_1.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_2.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_3.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_4.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_5.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_6.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_7.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_8.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_9.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/wisconsin/out1_node_feature_label.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texas()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/wisconsin/out1_graph_edges.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_0.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_1.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_2.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_3.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_4.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_5.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_6.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_7.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_8.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_9.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_node_feature_label.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wisconsin()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_graph_edges.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_0.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_1.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_2.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_3.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_4.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_5.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_6.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_7.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_8.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_9.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_node_feature_label.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikipediaNetwork()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_graph_edges.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_0.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_1.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_2.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_3.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_4.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_5.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_6.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_7.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_8.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_9.npz\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikipediaNetwork()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    \n",
    "    DIR = '/scratch/gilbreth/das90/Dataset//LINKXdataset'\n",
    "    \n",
    "#     DATASETS = [\"Roman-empire\", \"Amazon-ratings\", \"Minesweeper\", \"Tolokers\", \"Questions\"]\n",
    "    \n",
    "#     for d_name in DATASETS:        \n",
    "#         dataset = HeterophilousGraphDataset(root=DIR, name = d_name)        \n",
    "#         print(dataset)\n",
    "        \n",
    "    \n",
    "#     DATASETS = [\"penn94\", \"reed98\", \"amherst41\", \"cornell5\", \"johnshopkins55\", \"genius\"]\n",
    "    \n",
    "#     for d_name in DATASETS:        \n",
    "#         dataset = LINKXDataset(root=DIR, name = d_name)\n",
    "#         print(dataset)\n",
    "        \n",
    "    \n",
    "    \n",
    "#     dataset = Actor(root=DIR+'Actor')        \n",
    "    \n",
    "    DATASETS = [\"Cornell\", \"Texas\", \"Wisconsin\"]\n",
    "    \n",
    "    for d_name in DATASETS:        \n",
    "        dataset = WebKB(root=DIR, name = d_name)\n",
    "        print(dataset)\n",
    "    \n",
    "    DATASETS = [\"chameleon\", \"squirrel\"] #\"crocodile\",\n",
    "\n",
    "    for d_name in DATASETS:        \n",
    "        dataset = WikipediaNetwork(root=DIR, name = d_name)\n",
    "        print(dataset)\n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f304545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py38cu11 Kernel)",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
