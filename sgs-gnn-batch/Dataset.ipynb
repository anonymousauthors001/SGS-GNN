{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74d868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "kernel_name = os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))\n",
    "\n",
    "if kernel_name == 'py38cu11':\n",
    "    import ctypes\n",
    "    ctypes.cdll.LoadLibrary(\"/apps/gilbreth/cuda-toolkit/cuda-11.2.0/lib64/libcusparse.so.11\");\n",
    "    ctypes.cdll.LoadLibrary(\"/apps/gilbreth/cuda-toolkit/cuda-11.2.0/lib64/libcublas.so.11\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ec2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch_geometric.typing import Adj, SparseTensor\n",
    "from torch_geometric.utils import coalesce, degree\n",
    "# from torch_geometric.utils.to_dense_adj import to_dense_adj\n",
    "\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.datasets import Reddit, Flickr, Yelp, AmazonProducts, PPI,  OGB_MAG,  FakeDataset, Amazon, Coauthor\n",
    "# import torch_geometric.utils as homophily\n",
    "# import torch_geometric.utils as subgraph\n",
    "from torch_geometric.utils import assortativity, subgraph, homophily, to_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523668ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gamma, uniform, expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42962da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/SitaoLuan/ACM-GNN/tree/main/synthetic-experiments\n",
    "#https://github.com/KAIDI3270/Geom_GCN_pytorch_implementation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def func(feature):\n",
    "\n",
    "    f = list(map(int, feature.split(',')))\n",
    "    \n",
    "    return f\n",
    "\n",
    "def get_heterophily(root, DATASET_NAME='texas', train=0.6, val=0.2, test=0.2):\n",
    "    \n",
    "    edge_file = root+'/'+DATASET_NAME+'/out1_graph_edges.txt'\n",
    "    id_feature_label_file = root+'/'+DATASET_NAME+'/out1_node_feature_label.txt'\n",
    "    \n",
    "    edges = pd.read_csv(edge_file, sep='\\t', header=0)\n",
    "    id_feature_label = pd.read_csv(id_feature_label_file, sep='\\t', header=0)\n",
    "    \n",
    "#     print(edges)\n",
    "#     print(id_feature_label)\n",
    "    \n",
    "    edge_index = torch.LongTensor(edges.values.tolist()).T\n",
    "    node_id  = torch.LongTensor(id_feature_label['node_id'].values.tolist())\n",
    "    y = torch.LongTensor(id_feature_label['label'].values.tolist())\n",
    "    x = id_feature_label['feature'].apply(func)\n",
    "    x = torch.Tensor(x.values.tolist())\n",
    "    \n",
    "    N = len(node_id)\n",
    "    indexs = list(range(N))\n",
    "    \n",
    "    train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "    val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True    \n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    \n",
    "    data = Data(edge_index=edge_index, \n",
    "                x=x, node_id=node_id, \n",
    "                y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#get_heterophily('/scratch/gilbreth/das90/Dataset/heterophily/','squirrel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8f65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_film(root, DATASET_NAME='film', train=0.6, val=0.2, test=0.2):\n",
    "    \n",
    "    file = root+DATASET_NAME+'/'    \n",
    "    f = open(file+'class_map.json')\n",
    "    class_map = json.load(f)\n",
    "    class_map = {int(key):int(value) for key, value in class_map.items()}\n",
    "    #print(class_map)\n",
    "    f.close()    \n",
    "    \n",
    "    y = list(class_map.values())\n",
    "    x = np.load(file+'feats.npy')    \n",
    "    #print(x.shape)\n",
    "    \n",
    "#     f = open(file+'id_map.json')\n",
    "#     id_map = json.load(f)\n",
    "#     id_map = {int(key):int(value) for key, value in id_map.items()}    \n",
    "#     #print(id_map)\n",
    "#     f.close()\n",
    "    \n",
    "    #target = pd.read_csv(file+'film_target.csv', sep=',', header=0)\n",
    "    #print(target)\n",
    "    #target['new_id']=target['id'].apply(lambda x: id_map[x])\n",
    "    \n",
    "    \n",
    "    edges = pd.read_csv(file+'film_edges.csv', sep=',', header=0)\n",
    "    \n",
    "    u = edges['id1'].values.tolist()\n",
    "    v = edges['id2'].values.tolist()\n",
    "    \n",
    "    edge_index=[u,v]\n",
    "    \n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "    edge_index = torch.LongTensor(edge_index)\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    indexs = list(range(N))\n",
    "    train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "    val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "#     train_index, test_index = train_test_split(indexs, test_size=val+test)\n",
    "#     val_index, test_index = train_test_split(test_index, test_size=test/(val+test))\n",
    "    \n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True    \n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    \n",
    "    data = Data(edge_index=edge_index, \n",
    "                x=x,\n",
    "                y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#get_film('/scratch/gilbreth/das90/Dataset/heterophily/','film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f6acefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_mask(data, train=0.6, val=0.2, test=0.2, random_state=False):\n",
    "    \n",
    "    if isinstance(data.x, SparseTensor):\n",
    "        N = data.x.size(0)\n",
    "        data.num_nodes = N\n",
    "    else:\n",
    "        N = data.x.shape[0]\n",
    "    \n",
    "    indexs = list(range(N))\n",
    "    \n",
    "    if random_state:\n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test))\n",
    "    else:        \n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True    \n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba35d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDataset(Dataset):\n",
    "    def __init__(self, root, dataset_name, train=0.6, val=0.2, test=0.2,\n",
    "                 transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(None, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.root = root\n",
    "        self.dataset_name=dataset_name\n",
    "        self.degree=degree\n",
    "        self.train=train\n",
    "        self.val=val\n",
    "        self.test=test\n",
    "        \n",
    "        if dataset_name == 'film':\n",
    "            self.data = get_film(root,dataset_name, train, val, test)\n",
    "        else:\n",
    "            self.data = get_heterophily(root,dataset_name, train, val, test)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return self.dataset_name\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return self.root\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return self.data.x.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return max(self.data.y).item()+1\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "# dataset = HeteroDataset('/scratch/gilbreth/das90/Dataset/heterophily/','texas')\n",
    "# data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "501b6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LINKXpyg2(Dataset):\n",
    "    def __init__(self, root, dataset_name, train=0.6, val=0.2, test=0.2,\n",
    "                 transform=None, pre_transform=None, pre_filter=None, random_state=False):\n",
    "        super().__init__(None, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.root = root\n",
    "        self.dataset_name=dataset_name\n",
    "        self.degree=degree\n",
    "        self.train=train\n",
    "        self.val=val\n",
    "        self.test=test\n",
    "        \n",
    "        FolderName = root+'/LINKXdataset/'+dataset_name+'/'\n",
    "\n",
    "        data = Data()\n",
    "\n",
    "        data.x = torch.load(FolderName+'x.pt')\n",
    "        data.edge_index =torch.load(FolderName+'edge_index.pt')\n",
    "        data.y = torch.load(FolderName+'y.pt')\n",
    "    \n",
    "        self.data = train_val_test_mask(data, train=train, val=val, test=test, random_state=random_state)\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return self.dataset_name\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return self.root+'/LINKXdataset/'+self.dataset_name\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return self.data.x.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return max(self.data.y).item()+1\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "# #'pokec', 'arxiv-year', 'snap-patents', 'twitch-gamer'\n",
    "# dataset = LINKXpyg2('/scratch/gilbreth/das90/Dataset/','pokec', random_state=0)\n",
    "# data = dataset[0]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013ed21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OGB_MAGcustom(Dataset):\n",
    "    def __init__(self, root, dataset_name, data, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(None, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.dataset_name=dataset_name\n",
    "        \n",
    "        self.FolderName = root\n",
    "        self.data = data\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return self.dataset_name\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return self.root+self.dataset_name\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return self.data.x.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return max(self.data.y).item()+1\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):        \n",
    "        return self.data\n",
    "    \n",
    "# #'pokec', 'arxiv-year', 'snap-patents', 'twitch-gamer'\n",
    "# dataset = LINKXpyg2('/scratch/gilbreth/das90/Dataset/','pokec', random_state=0)\n",
    "# data = dataset[0]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e8e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIR='/scratch/gilbreth/das90/Dataset/'\n",
    "# dataset = Reddit(root=DIR+'Reddit')\n",
    "# data = dataset[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79d3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(DATASET_NAME='Cora', \n",
    "             DIR=None, params=None, \n",
    "             train=None, random_state=False, log=True, h_score=False, split_no=0):\n",
    "    \n",
    "    if DIR is not None:\n",
    "        if log: print('Looking at: ',DIR)    \n",
    "    elif os.uname()[1].find('gilbreth')==0: ##if not darwin(mac/locallaptop)\n",
    "        DIR='/scratch/gilbreth/das90/Dataset/'\n",
    "    elif os.uname()[1].find('unimodular')==0:\n",
    "        DIR='/scratch2/das90/Dataset/'\n",
    "    else:\n",
    "        DIR='./Dataset/'\n",
    "\n",
    "    Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    RESULTS_DIR=DIR+'RESULTS/'\n",
    "    Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if log:\n",
    "        print(\"Data directory: \", DIR)\n",
    "        print(\"Result directory:\", RESULTS_DIR)\n",
    "    \n",
    "    from torch_geometric.datasets import Planetoid,  KarateClub, CitationFull\n",
    "    from torch_geometric.transforms import NormalizeFeatures\n",
    "    from torch_geometric.datasets import Reddit, Reddit2\n",
    "    \n",
    "    #DATASET_NAME='Cora' #\"Cora\", \"CiteSeer\", \"PubMed\"\n",
    "\n",
    "    if DATASET_NAME in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
    "        dataset = Planetoid(root=DIR+'Planetoid', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME in ['cora', 'cora_ml', 'citeseer', 'dblp', 'pubmed']:\n",
    "        #['cora', 'cora_ml', 'citeseer', 'dblp', 'pubmed']\n",
    "        dataset = CitationFull(root=DIR+'Citation', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME == \"Reddit2\":\n",
    "        from ipynb.fs.full.RedditTwo import Reddit2\n",
    "        #dataset = Reddit2(root=DIR+'Reddit2', transform=NormalizeFeatures())\n",
    "        dataset = Reddit2(root=DIR+'Reddit2')\n",
    "        \n",
    "        \n",
    "    elif DATASET_NAME == \"Reddit\":\n",
    "        #dataset = Reddit(root=DIR+'Reddit', transform=NormalizeFeatures())\n",
    "        dataset = Reddit(root=DIR+'Reddit')    \n",
    "        \n",
    "    elif DATASET_NAME in [\"RedditSynthetic\", \"Reddit0.125\",\"Reddit0.225\",\"Reddit0.325\",\"Reddit0.425\",\"Reddit0.525\",\n",
    "                          \"Reddit0.625\",\"Reddit0.725\",\"Reddit0.825\",\"Reddit0.925\"]:\n",
    "        \n",
    "        h = k = 0\n",
    "        \n",
    "        if DATASET_NAME == \"RedditSynthetic\":\n",
    "            h = params['h']\n",
    "            k = params['k']\n",
    "        \n",
    "        else:\n",
    "            k = int(DATASET_NAME[-2:])\n",
    "            h = float(DATASET_NAME[-5:-2])\n",
    "            \n",
    "            #print(h,k)\n",
    "        \n",
    "        dataset, data = RedditSynthetic(DIR, h=h, k=k, log=True, recompute=False)\n",
    "                \n",
    "    \n",
    "    elif DATASET_NAME in ['ego-Gplus', 'gemsec-Facebook']:\n",
    "        from torch_geometric.datasets import SNAPDataset        \n",
    "        dataset = SNAPDataset(root=DIR+'SNAPDataset', name=DATASET_NAME, transform=NormalizeFeatures())        \n",
    "        print(dataset)\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "        \n",
    "    \n",
    "    elif DATASET_NAME in [\"BlogCatalog\", \"PPI\", \"Facebook\", \"Twitter\", \"TWeibo\", \"MAG\"]:\n",
    "        from torch_geometric.datasets import AttributedGraphDataset\n",
    "        \n",
    "#         dataset = AttributedGraphDataset(root=DIR+'/AttributedGraphDataset', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "        dataset = AttributedGraphDataset(root=DIR+'/AttributedGraphDataset', name=DATASET_NAME)\n",
    "        \n",
    "        print(dataset)\n",
    "        print(dataset[0])        \n",
    "        \n",
    "    elif DATASET_NAME == \"AmazonProducts\":\n",
    "        #dataset = AmazonProducts(root=DIR+'AmazonProducts', transform=NormalizeFeatures())\n",
    "        dataset = AmazonProducts(root=DIR+'AmazonProducts')\n",
    "        \n",
    "    elif DATASET_NAME in ['Computers', 'Photo']:\n",
    "        #dataset = Amazon(root=DIR+'AmazonProducts', transform=NormalizeFeatures())\n",
    "        dataset = Amazon(root=DIR+'Amazon/', name = DATASET_NAME)        \n",
    "        \n",
    "\n",
    "    elif DATASET_NAME in ['CS', 'Physics']:\n",
    "        dataset = Coauthor(root=DIR+'Coauthor/', name = DATASET_NAME)        \n",
    "        \n",
    "    elif DATASET_NAME in ['WikiCS']:\n",
    "        from torch_geometric.datasets import WikiCS\n",
    "        dataset = WikiCS(root=DIR+'WikiCS/', is_undirected=False)\n",
    "        dataset[0].train_mask = None\n",
    "        dataset[0].test_mask = None\n",
    "        dataset[0].val_mask = None\n",
    "\n",
    "    elif DATASET_NAME in ['ogbn-proteins']:\n",
    "        from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "        from torch_geometric.utils import scatter\n",
    "\n",
    "        dataset = PygNodePropPredDataset('ogbn-proteins', root=DIR+'/ogbn-proteins')        \n",
    "        \n",
    "    elif DATASET_NAME == \"Moon\":\n",
    "        dataset = MoonGraph.MoonDataset(n_samples=100, degree=5, train=0.5)    \n",
    "        G, data =dataset[0]\n",
    "    \n",
    "    elif DATASET_NAME == \"karate\":\n",
    "        dataset = KarateClub()        \n",
    "        data = dataset[0]\n",
    "        data.val_mask = ~data.train_mask\n",
    "        data.test_mask = data.val_mask\n",
    "    \n",
    "    elif DATASET_NAME == \"Fake\":\n",
    "        dataset = FakeDataset(num_graphs = 1, \n",
    "                              avg_num_nodes = 2000, \n",
    "                              avg_degree = 10, \n",
    "                              num_channels = 64, \n",
    "                              edge_dim = 0, \n",
    "                              num_classes = 10, \n",
    "                              task = 'auto', \n",
    "                              is_undirected = True,                               \n",
    "                              transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME == \"OGB_MAG\":\n",
    "        #dataset = OGB_MAG(root=DIR+'OGB_MAG', preprocess='metapath2vec', transform=NormalizeFeatures())\n",
    "        dataset = OGB_MAG(root=DIR+'OGB_MAG3', preprocess='metapath2vec')\n",
    "        \n",
    "        data = dataset[0]                \n",
    "        #print(dataset, data, data['paper'])        \n",
    "        data = Data(x=data['paper'].x, edge_index=data['paper', 'cites', 'paper'].edge_index,\n",
    "                   train_mask=data['paper'].train_mask,val_mask = data['paper'].val_mask,test_mask = data['paper'].test_mask,y= data['paper'].y,)\n",
    "        \n",
    "        dataset = OGB_MAGcustom(DIR+'OGB_MAG3','OGB_MAG',data)\n",
    "        \n",
    "        #return dataset\n",
    "        \n",
    "    elif DATASET_NAME == \"Flickr\":\n",
    "        dataset = Flickr(root=DIR+'Flickr')\n",
    "    \n",
    "    elif DATASET_NAME == \"Yelp\":\n",
    "        dataset = Yelp(root=DIR+'Yelp')\n",
    "    \n",
    "    elif DATASET_NAME == \"PPI\":\n",
    "        \n",
    "        dataset = PPI(root=DIR+'PPI')\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    ###heterophilic dataset\n",
    "    #https://github.com/pyg-team/pytorch_geometric/blob/master/examples/linkx.py\n",
    "    elif DATASET_NAME in ['pokec', 'arxiv-year', 'snap-patents', 'twitch-gamer','wiki']:\n",
    "        dataset = LINKXpyg2(DIR, DATASET_NAME, random_state=random_state)\n",
    "    \n",
    "    elif DATASET_NAME in [\"penn94\", \"reed98\", \"amherst41\", \"cornell5\", \"johnshopkins55\", \"genius\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import LINKXDataset\n",
    "        \n",
    "        dataset = LINKXDataset(root = DIR+'/Heterophilic/', name = DATASET_NAME)    \n",
    "        #transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME in [\"Roman-empire\", \"Amazon-ratings\", \"Minesweeper\", \"Tolokers\", \"Questions\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import HeterophilousGraphDataset\n",
    "        \n",
    "        dataset = HeterophilousGraphDataset(DIR+'/Heterophilic/', DATASET_NAME)    \n",
    "        #transform=NormalizeFeatures())\n",
    "    elif DATASET_NAME == 'Actor':\n",
    "        from ipynb.fs.full.HeterophilousDataset import Actor\n",
    "        \n",
    "        dataset = Actor(root=DIR+'/Heterophilic/Actor')\n",
    "        #transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME in [\"Cornell\", \"Texas\", \"Wisconsin\"]:\n",
    "        \n",
    "        \n",
    "        \n",
    "        from ipynb.fs.full.HeterophilousDataset import WebKB\n",
    "        \n",
    "        dataset = WebKB(root = DIR+'/Heterophilic/', name = DATASET_NAME)\n",
    "        #transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME in [\"Chameleon\", \"Crocodile\", \"Squirrel\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import WikipediaNetwork\n",
    "        \n",
    "        if DATASET_NAME == 'Crocodile':\n",
    "            dataset = WikipediaNetwork(DIR+'/Heterophilic/', DATASET_NAME.lower(), geom_gcn_preprocess= False)\n",
    "        else:        \n",
    "            dataset = WikipediaNetwork(DIR+'/Heterophilic/', DATASET_NAME.lower())\n",
    "            #transform=NormalizeFeatures())\n",
    "    \n",
    "    #implemented heterophily\n",
    "    elif DATASET_NAME in ['chameleon','cornell','film', 'squirrel', 'texas','wisconsin']:\n",
    "        dataset = HeteroDataset(DIR+'/heterophily/', DATASET_NAME)       \n",
    "    \n",
    "    else: \n",
    "        return None, None\n",
    "        raise Exception('dataset not found')\n",
    "\n",
    "    if DATASET_NAME in ['Moon', 'karate','OGB_MAG'] or DATASET_NAME[:7]=='Reddit0' or DATASET_NAME in ['RedditSynthetic']:\n",
    "        #MoonGraph.draw_blobs_data(G, data)        \n",
    "        None\n",
    "    \n",
    "    elif DATASET_NAME=='ogbn-proteins':\n",
    "        # splitted_idx = dataset.get_idx_split()\n",
    "        data = dataset[0]\n",
    "        data.node_species = None\n",
    "        data.y = data.y.to(torch.float)\n",
    "\n",
    "        # Initialize features of nodes by aggregating edge features.\n",
    "        row, col = data.edge_index\n",
    "        data.x = scatter(data.edge_attr, col, dim_size=data.num_nodes, reduce='sum')\n",
    "\n",
    "        # # Set split indices to masks.\n",
    "        # for split in ['train', 'valid', 'test']:\n",
    "        #     mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        #     mask[splitted_idx[split]] = True\n",
    "        #     data[f'{split}_mask'] = mask\n",
    "        \n",
    "        data = train_val_test_mask(data, train=0.6, val=0.2, test=0.2, random_state=random_state)\n",
    "    \n",
    "    elif DATASET_NAME == 'Reddit2':\n",
    "        \n",
    "        d = dataset[0][0]\n",
    "        data = Data(x = d['x'], y=d['y'], \n",
    "                    edge_index = d['edge_index'],\n",
    "                    train_mask=d['train_mask'],\n",
    "                    val_mask=d['val_mask'],\n",
    "                    test_mask = d['test_mask'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        data = dataset[0]  # Get the first graph object.\n",
    "        if 'train_mask' not in data:\n",
    "            data = train_val_test_mask(data, train=0.6, val=0.2, test=0.2, random_state=random_state)\n",
    "            \n",
    "        elif data.train_mask.dim()>1 and data.val_mask.dim()>1 and data.test_mask.dim()>1:\n",
    "            \n",
    "            if data.train_mask.shape[1]>split_no:            \n",
    "                data.train_mask = data.train_mask[:,split_no]\n",
    "                data.val_mask = data.val_mask[:,split_no]\n",
    "                data.test_mask = data.test_mask[:,split_no]\n",
    "            else:\n",
    "                data = train_val_test_mask(data, train=0.6, val=0.2, test=0.2, random_state=random_state)\n",
    "            \n",
    "        \n",
    "    if train is not None:\n",
    "        val = (1-train)/2.0\n",
    "        data = train_val_test_mask(data, train=train, val=val, test=1-(train+val), random_state=random_state)\n",
    "        \n",
    "    if log:\n",
    "        print()\n",
    "        print(f'Dataset: {dataset}:')\n",
    "        print('======================')\n",
    "        print(f'Number of graphs: {len(dataset)}')\n",
    "        print(f'Number of features: {dataset.num_features}')\n",
    "        print(f'Number of classes: {dataset.num_classes}')\n",
    "        print()\n",
    "        print(data)\n",
    "        print('===========================================================================================================')\n",
    "\n",
    "        # Gather some statistics about the graph.\n",
    "        print(f'Number of nodes: {data.num_nodes}')\n",
    "        print(f'Number of edges: {data.num_edges}')\n",
    "        print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "        print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "        print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "        print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "        print(f'Has self-loops: {data.has_self_loops()}')\n",
    "        print(f'Is undirected: {data.is_undirected()}')\n",
    "    \n",
    "    if len(data.y.shape) == 1:\n",
    "        labels = data.y\n",
    "    else:\n",
    "        if log: print(\"Testing homophily by converting multi-label to one-label\")\n",
    "        labels = data.y.argmax(dim=1)\n",
    "        data.y = labels\n",
    "    \n",
    "    if torch.min(data.y)<0:\n",
    "        if log: print(\"Shifting label to non-negative\")\n",
    "        data.y = data.y-torch.min(data.y)\n",
    "    \n",
    "    \n",
    "    if h_score:\n",
    "        print(\"N \",data.num_nodes, \" E \",data.num_edges,\" d \",data.num_edges / data.num_nodes, end=' ')\n",
    "        \n",
    "        print(homophily(data.edge_index, labels, method='node'),homophily(data.edge_index, labels, method='edge'), end=' ')\n",
    "        \n",
    "        try:\n",
    "            esen = homophily(data.edge_index, labels, method='edge_insensitive')\n",
    "        except:\n",
    "            esen = -1        \n",
    "        print(esen, end=' ')            \n",
    "        print(assortativity(data.edge_index), end=' ')\n",
    "        \n",
    "        \n",
    "    return data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bc434e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RedditSynthetic(DIR, h=0.5, k=25, log=False, recompute=False):\n",
    "    \n",
    "    file_path = DIR+'RedditSynthetic/Reddit'+str(h)+str(k)    \n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    dataset = Reddit(root=DIR+'Reddit')\n",
    "    \n",
    "    if os.path.isfile(file_path) and recompute==False:\n",
    "        data = torch.load(file_path)  \n",
    "        \n",
    "        if log:\n",
    "            print(\"loaded from: \",file_path)\n",
    "        \n",
    "        return dataset, data\n",
    "    \n",
    "    data = dataset[0]\n",
    "    \n",
    "    if log:\n",
    "        print(data)\n",
    "        \n",
    "    num_class = max(data.y)+1\n",
    "    \n",
    "    E = data.num_edges\n",
    "    N = data.num_nodes\n",
    "    \n",
    "    adj = SparseTensor(\n",
    "        row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    edge_numbers = []\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "    \n",
    "    \n",
    "    for i in range(N):\n",
    "        row, col, edge = adj[i,:].coo()      \n",
    "        \n",
    "        if len(col) == 0: \n",
    "            if log:\n",
    "                pbar.update(1)\n",
    "            continue\n",
    "        y_current = data.y[i].item()\n",
    "        \n",
    "#         print(y_current)\n",
    "#         print(row, col, edge)\n",
    "        \n",
    "        \n",
    "        match_indexes = ((data.y[col] == y_current).nonzero()).view(-1).numpy()\n",
    "        other_indexes = ((data.y[col] != y_current).nonzero()).view(-1).numpy()\n",
    "        \n",
    "#         print(len(match_indexes),match_indexes)\n",
    "#         print(len(other_indexes),other_indexes)\n",
    "        \n",
    "    \n",
    "        select=int(len(col)*k/100) #select k percent of nodes\n",
    "        h_select = min(int(select*h),len(match_indexes)) #select h homophilic nodes\n",
    "        o_select = min(select-h_select, len(other_indexes))\n",
    "        \n",
    "#         print(h_select, len(match_indexes))\n",
    "#         print(o_select, len(other_indexes))\n",
    "#         print(\"*\"*100)\n",
    "        \n",
    "        samples1 = np.random.choice(match_indexes, h_select, replace=False)\n",
    "        samples2 = np.random.choice(other_indexes, o_select, replace=False)\n",
    "                            \n",
    "        edge_numbers.append(edge[samples1])\n",
    "        edge_numbers.append(edge[samples2])\n",
    "        \n",
    "        if log:\n",
    "            pbar.update(1)\n",
    "    if log:        \n",
    "        pbar.close()\n",
    "            \n",
    "    edge_numbers = torch.cat(edge_numbers)    \n",
    "    edge_index = data.edge_index[:,edge_numbers]\n",
    "    \n",
    "    if log:\n",
    "        print(f'Average node degree: {edge_index.shape[1] / N:.2f}')    \n",
    "        print(homophily(edge_index, data.y, method='node'),homophily(edge_index, data.y, method='edge'), end=' ')\n",
    "    \n",
    "    data.edge_index = edge_index\n",
    "    \n",
    "    torch.save(data, file_path)\n",
    "\n",
    "    if log:\n",
    "        print(\"Data saved to file: \",file_path)\n",
    "        \n",
    "    return dataset, data\n",
    "\n",
    "# RedditSynthetic(DIR, h=0.5, k=25, log=True, recompute=False)\n",
    "\n",
    "# for h in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "#     RedditSynthetic(DIR, h=h, k=25, log=True, recompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b34b4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import SNAPDataset\n",
    "# DIR='/scratch/gilbreth/das90/Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4edc89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datasets = [\n",
    "    \"Texas\",\n",
    "    \"Cora\",\n",
    "    \"Reddit\",\n",
    "    \"AmazonProducts\",\n",
    "    \n",
    "    \"Photo\",\n",
    "    \"WikiCS\",\n",
    "    \"Reddit2\",\n",
    "    #\"Amazon2M\",\n",
    "    \n",
    "    \"Squirrel\",\n",
    "    \"penn94\",\n",
    "    \"ogbn-proteins\",\n",
    "    \"pokec\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dd6bb4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "\n",
      "Dataset: KarateClub():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 34\n",
      "Number of classes: 4\n",
      "\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34])\n",
      "===========================================================================================================\n",
      "Number of nodes: 34\n",
      "Number of edges: 156\n",
      "Average node degree: 4.59\n",
      "Number of training nodes: 4\n",
      "Training node label rate: 0.12\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "N  34  E  156  d  4.588235294117647 0.8020520210266113 0.7564102411270142 0.6170591711997986 -0.4756128787994385 "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':   \n",
    "    data, dataset = get_data('karate', log=True, h_score = True, split_no = 0)\n",
    "\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc9f2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('ogbn-proteins', log=True, h_score = True, split_no = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b338c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af4cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1d0ffb4",
   "metadata": {},
   "source": [
    "## Dataset Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02ffb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"Cornell\",\n",
    "    \"Texas\",\n",
    "    \"Wisconsin\",\n",
    "    \"reed98\",\n",
    "    \"amherst41\",\n",
    "    \"penn94\",\n",
    "    \"Roman-empire\",\n",
    "    \"cornell5\",\n",
    "    \"Squirrel\",\n",
    "    \"johnshopkins55\",\n",
    "#     \"AmazonProducts\", #error\n",
    "    \"Actor\",\n",
    "    \"Minesweeper\",\n",
    "    \"Questions\",\n",
    "    \"Chameleon\",\n",
    "    \"Tolokers\",\n",
    "#     \"Flickr\", #error\n",
    "#     \"Yelp\", #error\n",
    "    \"Amazon-ratings\",\n",
    "    \"genius\",\n",
    "    \"cora\",\n",
    "    \"CiteSeer\",\n",
    "    \"dblp\",\n",
    "    \"Computers\",\n",
    "    \"pubmed\",\n",
    "    \"Reddit\",\n",
    "    \"cora_ml\",\n",
    "    \"Cora\",\n",
    "#     \"Reddit2\", #error\n",
    "    \"CS\",\n",
    "    \"Photo\",\n",
    "    \"Physics\",\n",
    "    \"citeseer\",    \n",
    "    'pokec',\n",
    "    'arxiv-year',\n",
    "#     'snap-patents', #error\n",
    "#     'twitch-gamer', #error\n",
    "#     'wiki', #error\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fac98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d91b7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_homophily(data,num_classes,log=True):\n",
    "    h_edge = homophily(data.edge_index, data.y, method='edge')\n",
    "    h_adj = -1\n",
    "    E = data.num_edges\n",
    "    N = data.num_nodes\n",
    "    \n",
    "    adj = SparseTensor(\n",
    "        row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    class_degree=np.zeros(num_classes)\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "        \n",
    "    for i in range(N):\n",
    "        row, col, edge = adj[i,:].coo()      \n",
    "        \n",
    "        if len(col) == 0: \n",
    "            if log:\n",
    "                pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        y_current = data.y[i].item()\n",
    "        class_degree[y_current] += len(col)        \n",
    "        \n",
    "        if log:\n",
    "            pbar.update(1)\n",
    "    if log:        \n",
    "        pbar.close()\n",
    "        \n",
    "    D_k = np.sum(class_degree**2)/E**2\n",
    "    \n",
    "    #print(D_k)\n",
    "    \n",
    "    h_adj = (h_edge - D_k)/(1-D_k)\n",
    "    \n",
    "    return h_adj\n",
    "\n",
    "# data, dataset = get_data('Tolokers', log=False, h_score = False)\n",
    "# adj_homophily(data, dataset.num_classes, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaa0c1d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataset_properties():\n",
    "    \n",
    "    for i, dataset in enumerate(new_datasets):\n",
    "        print(dataset, end='\\t')\n",
    "#         print(i,\"\\t\",dataset)\n",
    "        data, dataset = get_data(dataset, log=False, h_score = False)\n",
    "                \n",
    "        if len(data.y.shape) > 1:\n",
    "            data.y = data.y.argmax(dim=1)        \n",
    "            num_classes = torch.max(data.y).item()+1\n",
    "        else:\n",
    "            #num_classes = dataset.num_classes      \n",
    "            num_classes = torch.max(data.y).item()+1\n",
    "        \n",
    "        \n",
    "        tr = int(data.train_mask.sum()) / data.num_nodes\n",
    "        va = int(data.val_mask.sum()) / data.num_nodes\n",
    "        te = int(data.test_mask.sum()) / data.num_nodes\n",
    "        \n",
    "        print(f'tr/va/te {tr:0.2f}/{va:0.2f}/{te:0.2f}', end=' ')\n",
    "\n",
    "\n",
    "        f = dataset.num_features\n",
    "        c = num_classes\n",
    "        i = \"Yes\" if data.has_isolated_nodes() else \"No\"\n",
    "        sl = \"Yes\" if data.has_self_loops() else \"No\"\n",
    "        direc = \"Yes\" if data.is_undirected() else \"No\"\n",
    "        print(f, \" \", c, \" \", i, \" \",sl, \" \", direc, end=' ')\n",
    "        \n",
    "        h_adj = adj_homophily(data, num_classes, log=False)        \n",
    "        print('h_adj ',h_adj)\n",
    "        \n",
    "    return \n",
    "    \n",
    "    \n",
    "# dataset_properties()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c29b0",
   "metadata": {},
   "source": [
    "# Plot homophily Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "187afc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_compute(data):\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "    \n",
    "    adj = SparseTensor(\n",
    "        row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    hp_data=np.zeros(N)\n",
    "    \n",
    "    pbar = tqdm(total=N)\n",
    "    pbar.set_description(f'Nodes')\n",
    "        \n",
    "    for i in range(N):\n",
    "        row, col, edge = adj[i,:].coo()      \n",
    "        \n",
    "        if len(col) == 0: \n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        y_current = data.y[i]\n",
    "        y_neighbors = data.y[col]\n",
    "        \n",
    "        match  = (y_neighbors==y_current).type(torch.int).sum()\n",
    "        \n",
    "        hp_data[i] = match.item()/len(y_neighbors)\n",
    "        \n",
    "        #print(y_current, y_neighbors, match, hp_data[i])\n",
    "        \n",
    "        pbar.update(1)\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    return hp_data\n",
    "\n",
    "#hp_data = hp_compute(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b428401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter, StrMethodFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c7d5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rcParams[\"font.family\"] = 'DeJavu Serif'\n",
    "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]\n",
    "plt.rcParams[\"font.size\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16e102dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/matplotlib/matplotlib/issues/5862#issuecomment-197330145\n",
    "def fix_eps(fpath):\n",
    "    \"\"\"Fix carriage returns in EPS files caused by Arial font.\"\"\"\n",
    "    txt = b\"\"\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        for line in f:\n",
    "            if b\"\\r\\rHebrew\" in line:\n",
    "                line = line.replace(b\"\\r\\rHebrew\", b\"Hebrew\")\n",
    "            txt += line\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        f.write(txt)\n",
    "            \n",
    "def pd_hist(data, DATASET_NAME=''):\n",
    "    \n",
    "#     plt.rcParams[\"font.family\"] = \"serif\"\n",
    "#     plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]\n",
    "    \n",
    "    width = 5\n",
    "    font_size = 16\n",
    "    \n",
    "    \n",
    "    plt.rc('font', size=font_size)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=font_size)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=font_size)     # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=font_size)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=font_size)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=font_size)    # legend fontsize\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize = (5, 5), dpi=150);\n",
    "    ax = plt.gca();\n",
    "    ax.set_aspect('auto')\n",
    "    fig.canvas.draw();      \n",
    "    \n",
    "    # Generate some random data\n",
    "    #data = np.random.normal(size=1000)\n",
    "    # Calculate the probability density function\n",
    "    density, bins, _ = plt.hist(data, density=False, bins=25)\n",
    "\n",
    "    # Plot the probability density function\n",
    "    plt.plot(bins[:-1], density)\n",
    "    \n",
    "    font = {'fontname':'Times New Roman', 'size':font_size}\n",
    "    \n",
    "    #ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Local Node homophily')\n",
    "    plt.ylabel('Number of Nodes')\n",
    "    #plt.title('Histogram of node homophily values'+': '+DATASET_NAME)\n",
    "#     plt.ticklabel_format(axis='y', style='sci', scilimits=(0,1),useMathText=True)\n",
    "\n",
    "    class MathTextSciFormatter(ticker.Formatter):\n",
    "        def __init__(self, fmt=\"%1.1e\"):\n",
    "            self.fmt = fmt\n",
    "        def __call__(self, x, pos=None):\n",
    "            s = self.fmt % x\n",
    "            decimal_point = '.'\n",
    "            positive_sign = '+'\n",
    "            tup = s.split('e')\n",
    "            significand = tup[0].rstrip(decimal_point)\n",
    "            sign = tup[1][0].replace(positive_sign, '')\n",
    "            exponent = tup[1][1:].lstrip('0')\n",
    "            if exponent:\n",
    "                exponent = '10^{%s%s}' % (sign, exponent)\n",
    "            if significand and exponent:\n",
    "                s =  r'%s{\\times}%s' % (significand, exponent)\n",
    "            else:\n",
    "                s =  r'%s%s' % (significand, exponent)\n",
    "            return \"${}$\".format(s)\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(MathTextSciFormatter(\"%1.1e\"))\n",
    "\n",
    "\n",
    "    filename=\"Plots/homophily_\"+DATASET_NAME\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show();\n",
    "    fig.savefig(filename + '.pdf', format = 'pdf', bbox_inches='tight');\n",
    "    fig.savefig(filename + '.eps', format = 'eps', bbox_inches='tight', dpi = fig.dpi);\n",
    "    fix_eps(filename + '.eps');\n",
    "    \n",
    "    return \n",
    "\n",
    "# hp_data = [0.01, 0.1,0.6]\n",
    "# pd_hist(hp_data, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59ad726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # datasets = ['Cora', 'karate']\n",
    "# for DATASET_NAME in datasets:\n",
    "#     data, dataset = get_data(DATASET_NAME, log=False, h_score = False)\n",
    "#     hp_data = hp_compute(data)\n",
    "#     pd_hist(hp_data, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1054268",
   "metadata": {},
   "source": [
    "## Generate Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c300ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_class(data):\n",
    "    \n",
    "    unique_elements, counts = torch.unique(data.y, return_counts=True)\n",
    "#     print(unique_elements)\n",
    "#     print(counts)\n",
    "    mincount = min(counts).item()\n",
    "#     print(mincount)\n",
    "\n",
    "    subset = []\n",
    "\n",
    "    for i in unique_elements:\n",
    "        indexes = ((data.y == i).nonzero()).view(-1).numpy()\n",
    "#         print(len(indexes))\n",
    "#         print(indexes)\n",
    "        samples = np.random.choice(indexes, mincount, replace=False)\n",
    "        subset.extend(samples)\n",
    "\n",
    "#     print(len(subset), mincount*num_classes)\n",
    "\n",
    "    node_idx = torch.tensor(subset)\n",
    "    edge_index = subgraph(node_idx, data.edge_index)[0]\n",
    "    \n",
    "#     print(node_idx, edge_index)\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "\n",
    "    data.num_nodes = node_idx.size(0)\n",
    "    data.edge_index = edge_index\n",
    "\n",
    "    for key, item in data:\n",
    "        if key in ['edge_index', 'num_nodes']:\n",
    "            continue\n",
    "        if isinstance(item, torch.Tensor) and item.size(0) == N:\n",
    "            data[key] = item[node_idx]\n",
    "        elif isinstance(item, torch.Tensor) and item.size(0) == E:\n",
    "            data[key] = item[edge_idx]\n",
    "        else:\n",
    "            data[key] = item\n",
    "    \n",
    "    return data\n",
    "\n",
    "# print(data.edge_index)\n",
    "# data = balance_class(data)\n",
    "# print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cef9621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N  2708  E  10556  d  3.8980797636632203 0.825157880783081 0.8099659085273743 0.7657181620597839 -0.06587088108062744 d=10.0000 Hn=0.0000 He=0.0000 Hin=0.0000 a=inf c= [0, 1, 2, 3, 4, 5, 6] [180, 180, 180, 180, 180, 180, 180] #c=0.2000 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[1260, 1433], edge_index=[2, 12600], y=[1260], train_mask=[1260], val_mask=[1260], test_mask=[1260], num_nodes=1260)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_synthetic(data, d=5, h=0.8, train=0.6, random_state=None, log=True, balance = False):\n",
    "    \n",
    "    if balance:\n",
    "        data = balance_class(data)\n",
    "        \n",
    "    num_class = max(data.y)+1\n",
    "    cluster_vertices = {}\n",
    "    for c in range(num_class):\n",
    "        indices = torch.where(data.y == c)[0]\n",
    "        cluster_vertices[c]=indices\n",
    "    \n",
    "    n = data.num_nodes\n",
    "    \n",
    "#     intra_d = np.random.multinomial(n*d*h, np.ones(n)/n, size=1)[0]\n",
    "#     inter_d = np.random.multinomial(n*d*(1-h), np.ones(n)/n, size=1)[0]\n",
    "    \n",
    "    intra_d = np.round(np.ones(n)*(d*h)).astype(int)\n",
    "    inter_d = np.round(np.ones(n)*(d*(1-h))).astype(int)\n",
    "    \n",
    "#     print(intra_d, inter_d)\n",
    "    \n",
    "    edge_index = [[],[]]\n",
    "    \n",
    "    for c in range(num_class):\n",
    "        intra_vertices = cluster_vertices[c]\n",
    "        inter_vertices = torch.cat([value for key, value in cluster_vertices.items() if key!=c])\n",
    "        \n",
    "        intra_vertices = intra_vertices.numpy()\n",
    "        inter_vertices = inter_vertices.numpy()\n",
    "        \n",
    "#         print('Class:', c)\n",
    "#         print(intra_vertices)\n",
    "#         print(inter_vertices)\n",
    "        \n",
    "        for u in intra_vertices:\n",
    "            \n",
    "            ## remove self-loop\n",
    "            #intra_vertices_u = \n",
    "            \n",
    "            intra_v = np.random.choice(intra_vertices, min(len(intra_vertices),intra_d[u]), replace=False)\n",
    "            inter_v = np.random.choice(inter_vertices, min(len(inter_vertices),inter_d[u]), replace=False)\n",
    "            \n",
    "            Vs = np.append(intra_v,inter_v)\n",
    "            Us = np.repeat(u,len(Vs))\n",
    "            \n",
    "            unique_elements, counts = np.unique(inter_v, return_counts=True)\n",
    "            \n",
    "#             print(\"-\"*50)\n",
    "#             print(u)\n",
    "#             print(Vs)\n",
    "#             print(unique_elements)\n",
    "#             print(counts)\n",
    "#             print(\"-\"*50)\n",
    "            \n",
    "#             if len(unique_elements)< (num_class-1):\n",
    "#                 print('un du toa:')\n",
    "#                 print(unique_elements)\n",
    "#                 print(counts)\n",
    "            \n",
    "            edge_index[0].extend(Us)\n",
    "            edge_index[1].extend(Vs)\n",
    "             \n",
    "#             edge_index[1].extend(Us)\n",
    "#             edge_index[0].extend(Vs)\n",
    "    \n",
    "    data.edge_index = torch.LongTensor(edge_index)\n",
    "    \n",
    "    if train is not None:\n",
    "        val = (1-train)/2.0\n",
    "        data = train_val_test_mask(data, train=train, val=val, test=1-(train+val), random_state=random_state)\n",
    "    \n",
    "    if log:\n",
    "        print(f\"d={data.num_edges/data.num_nodes:0.4f}\",end=' ')\n",
    "        print(f\"Hn={homophily(data.edge_index, data.y, method='node'):0.4f}\",end=' ')\n",
    "        print(f\"He={homophily(data.edge_index, data.y, method='edge'):0.4f}\",end=' ')\n",
    "        print(f\"Hin={homophily(data.edge_index, data.y, method='edge_insensitive'):0.4f}\",end=' ')\n",
    "        print(f\"a={assortativity(data.edge_index):0.4f}\", end=' ')\n",
    "        \n",
    "        unique_elements, counts = torch.unique(data.y, return_counts=True)\n",
    "        print(\"c=\",unique_elements.tolist(), counts.tolist(), end=' ')\n",
    "\n",
    "        print(f'#c={int(data.train_mask.sum()) / data.num_nodes:.4f}', end=' ')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# data, dataset = get_data('Cora', log=False, h_score = True, split_no = 0)\n",
    "# data = generate_synthetic(data, d=10, h=0.0, train=0.2, random_state=None, log=True, balance = True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed600e8",
   "metadata": {},
   "source": [
    "## Small Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e465dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "# y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "# edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "# edge_index = edge_index-1\n",
    "# data = Data(x=x, y=y, edge_index = edge_index)\n",
    "# draw_graph(edge_index, y, 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py311cu117pyg200 Kernel)",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
